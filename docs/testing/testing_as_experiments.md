# Testing as scientific experiments

If you are a scientist, or at least interested in the subject, you know about
the scientific method.

Simplified, the process works as follows:
  1. You make a number of observations.
  1. You formulate a hypothesis that has predictive power.
  1. You design experiments to test predictions made by the hypothesis.
  1. If an experiment succeeds, your confidence in the hypothesis increases.
  1. However, if an experiment fails, you know for certain that there is a
     problem and you reformulate your hypothesis, possibly gathering some more
     observations to do so.

It is worth pointing out that experiments are actually designed to disprove the
hypothesis rather than to confirm it. A quote of the philosopher of science Karl
Popper illustrates this.

If we are uncritical we shall always find what we want: we shall look for, and
find, confirmations, and we shall look away from, and not see, whatever might be
dangerous to our pet theories.

If you substitute "pet theories" in the above by "pet implementations", it is
quite clear that the same maxims apply to software testing as to scientific
research.

Another quote from his famous book "The logic of scientific discovery" (1934)
also translates well to software testing.

...no matter how many instances of white swans we may have observed, this does
not justify the conclusion that all swans are white.

Paraphrased, you can read that as "... no matter how many tests your software
may pass, it doesn't justify the conclusion that it is correct".
