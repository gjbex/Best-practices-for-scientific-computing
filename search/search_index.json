{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Best practices for scientific computing Material for a training on best practices for scientific computing. Programming languages Although this training aims to be programming language-agnostic, the repository also list a number of tools that are programming language-specific. Obviously, this can not be exhaustive, so feel free to suggest additional tools if you are aware of any. Programming languages covered: C C++ Fortran Python R Table of contents Syntax versus semantics Code style and conventions Version control & collaboration Code documentation Testing Testing as experiments Unit testing Functional testing Code coverage Optimization Deployment Continuous integration Reproducibility References Tools C C++ Fortran Python R Training","title":"Home"},{"location":"#best-practices-for-scientific-computing","text":"Material for a training on best practices for scientific computing.","title":"Best practices for scientific computing"},{"location":"#programming-languages","text":"Although this training aims to be programming language-agnostic, the repository also list a number of tools that are programming language-specific. Obviously, this can not be exhaustive, so feel free to suggest additional tools if you are aware of any. Programming languages covered: C C++ Fortran Python R","title":"Programming languages"},{"location":"#table-of-contents","text":"Syntax versus semantics Code style and conventions Version control & collaboration Code documentation Testing Testing as experiments Unit testing Functional testing Code coverage Optimization Deployment Continuous integration Reproducibility References Tools C C++ Fortran Python R Training","title":"Table of contents"},{"location":"about/","text":"About This material is licensed under Creative Commons . Acknowledgments I've \"borrowed\" much of the table of contents from a training given by the Netherlands eScience Center, although no actual contents of that training was used for the development of this material.","title":"About"},{"location":"about/#about","text":"This material is licensed under Creative Commons .","title":"About"},{"location":"about/#acknowledgments","text":"I've \"borrowed\" much of the table of contents from a training given by the Netherlands eScience Center, although no actual contents of that training was used for the development of this material.","title":"Acknowledgments"},{"location":"code_style/","text":"Code style A number of very simple things go a long way towards improving your code substantially. For good programmers, they are second nature, and you should strive to make them a habit. This text tries to be programming language-agnostic, and code fragments will be presented in several programming languages such as C, C++, Fortran and Python. However, even if you don't master these languages, the code fragments should be easy enough to understand. Although each programming languages has some specific best practices, many are applicable to any programming language. This is what this text focuses on. We also provide a list of references to best practices specific to various programming languages, and we encourage you strongly to read those as well. In this section, we will use the term function in a very broad sense, simply to keep the text easy to read. In the context of Fortran, a function refers to a program unit, any procedure, either a function or a subroutine. It also refers to a method defined in a class. In the context of Python and C++, we will use the term function for methods as well. Similarly, we use the term variable for constants, and also for attributes of objects and classes, whenever that doesn't lead to confusion. Of course, each programming language has its own style guides, often even several, you can find links to those in the reference section . Format your code nicely To quote Robert C. Martin, \"Code formatting is about communication, and communication is the professional developer\u2019s first order of business\". All programming languages have one or possibly multiple conventions about how to format source code. For example, consistent indentation of code helps considerably to assess the structure of a function at a glance. For multi-level indentation, always use the same width, e.g., multiples of four spaces. The convention you have to use is often determined by the community you are working with, such as your co-workers. It is best to stick to that convention, since coding is communicating. If no convention is established, consider introducing one. The one which is prevalent in the programming language community is most likely to be your best choice. For several programming languages there are tools to automatically format your code so that it adheres to a convention. It is considered good practice to use such tools if they are available, and even to make them part of your development pipeline, either as git pre-commit hooks, or as part of a more substantial CI/CD setup on GitHub or GitLab. Another issue is code formatting is the maximum number of characters on a line. Some guidelines for programmers have very strong opinions on that, while other are more tolerant. Since many editors will automatically display a view on your code with lines wrapped to the length that can be displayed on the screen, that might not seem an issue, but it can be argued that this makes the code harder to read. If you switch that off, you will have a view on your code were some lines are truncated, again a very unpleasant and potentially confusing situation. Personally I like the maximum line length to be 80 characters, since that will guarantee it can be displayed without the need to wrap it, even when displaying multiple editor windows side-by-side. Again, code formatters can enforce this and properly wrap the code for you automatically. Whichever convention you follow, be consistent! Use language idioms Linguists use the term \"idiom\" for an expression that is very specific to a certain language and that cannot be translated literally to another. For instance, the English idiom \"it is raining cats and dogs\" would translate to \"il pleut des cordes\" in French. The corresponding idiom in French is completely unrelated to its counterpart in English. Mastering idioms is one of the requirements for C1 certification, i.e., to be considered to have a proficiency close to that of native speakers. We observe a similar phenomenon for programming languages. Some syntactic constructs are typical for a specific programming language but, when translated one-to-one into another language, lead to code constructs that are unfamiliar to programmers who are proficient in that language. The code fragments below illustrate this for Fortran and C. Although you could write line 4 of the C function below in this way, you most likely wouldn't since it is not idiomatic C. int factorial(int n) { fac = 1; for (int i = 2; i <= n; i++) fac = fac*i; return fac; } The idiomatic formulation of line 4 would be fac *= i . In Fortran for example, you would write real, dimension(10) :: a ... a = value rather than integer :: i real, dimension(10) :: a ... do i = 1, 10 a(i) = value end do Using idioms, i.e., expressions that are particular to a (programming) language, will make your code much easier to interpret correctly by programmers that are fluent in that language. In the spirit of Robert C. Martin's quote: it is all about communication. However, there are sometimes different reasons as well. If we consider the following Python code that uses the numpy library, we can observe a marked performance difference when comparing the two code fragments below. a = np.emptylike(b) for i in range(b.shape[0]): for j in range(b.shape[1]): a[i, j] = np.sqrt(b[i, j]) versus a = np.sqrt(b) The second form is more idiomatic, but it will also substantially outperform the first fragment. MATLAB users will be aware of this difference as well. Idioms for programming languages are usually expressed in code style guidelines. You can find links to those in the reference section . Choose descriptive names In a way, programming is storytelling. The data are the protagonists in the story, and the functions are the actions they take, or what happens to them. Hence variable names should be nouns and functions names should be verbs. If a function returns a property, it should be phrased as a question. Any editor worth its salt provides completion, so you can't argue in favor of short but less descriptive names to save typing. A long but descriptive name is just hitting the tab key away. Choosing descriptive names for variables and functions is another aspect that can make reading your code much easier. Consider the following pseudo-code fragment, and although I'll grant that it is something of a caricature, I've seen some in the wild that are not significantly better. f = open(fn, 'r') for i in f: x = get(i) if condition(x): a = compute(x) if a < 3.14: do_something(a) f.close() A key principle of good software design is that of the least surprise . Choosing appropriate names for our variables and functions helps a lot in this respect. When it comes to programming languages, there is a very clear bias: all those I'm aware of have keywords defined in English. Hence I would argue that code that also uses English variable and function names is easier to read. This saves you from continuously, although unconsciously, to switch from one natural language to another. It may seem convenient, or even more natural to choose variable names in your native language, but that leads to more \"surprises\" when reading the code since you would also expect keywords in your native language to form actual sentences. Moreover, communicating about your code with others that don't speak your language will be just that bit harder. Just to put things in context: sometimes short variable names are perfectly adequate. For instance, trivial variable using in an iteration to index an array are conventionally called i , j and so on. This is perfectly fine, and unless you have a good reason to choose different names, the \"default\" names will even help others to understand your code. Also domain specific variables can have short names, using, e.g., T to denote the temperature, or t for time. This particular example would of course break for programming languages that are case-insensitive, e.g., Fortran. Note that these last remarks in no way contradicts the message of this section: T and t are very descriptive for developers in the domains where this notation is used. Keep it simple Ideally, code is simple. A function should have two levels of indentation at most. This is advice you'll find in the literature on general purpose programming. Although this is good advice, there are some caveats in the context of scientific computing. However, the gist is clear: code is as simple as possible, but not simpler. Even for scientific code, a function has no more lines of code than fit comfortably on your screen. It is all too easy to lose track of the semantics if you can't get an overview of the code. Remember, not everyone has the budget for a 5K monitor. If you find yourself writing a very long code fragment, ask yourself whether that is atomic, or whether the task it represents can be broken up into sub-tasks. If so, and that is very likely, introduce new functions for those sub-tasks with descriptive names. This will make the narrative all the easier to understand. A function should have a single purpose, i.e., you should design it to do one thing, and one thing only. For function signatures, simplicity matters as well. Functions that take many arguments may lead to confusion. In C and C++, you have to remember the order of the function arguments. Accidentally swapping argument values with the same type in a function call can lead to interesting debugging sessions. The same advice applies to Fortran procedures or Python functions, keep the number of arguments limited. However, both Fortran and Python support using keyword arguments, a nice feature that makes your code more robust. Consider the following procedure signature: real function random_gaussian(mu, sigma) implicit none real, intent(in) :: mu, sigma ... end function random_gaussian You would have to check the documentation to know the order of the function arguments. Consider the following four function calls: random_gaussian(0.0, 1.0) : okay; random_gaussian(1.0, 0.0) : not okay; random_gaussian(mu=0.0, sigma=1.0) : okay; random_gaussian(sigma=1.0, mu=0.0) : okay. The two last versions of this call are easier to understand, since the meaning of the numbers is clear. Moreover, since you can use any order, it eliminates a source of bugs. Unfortunately, neither C nor C++ support this feature. Limit scope Many programmers will declare all variables at the start of a block, or even at the start of a function's implementation. This is a syntax requirement in C89 and Fortran. However, C99, C++, R and Python allow you to declare variables anywhere before their first use. Since the scope of a variable starts from its declaration, and extends throughout the block, that means it is in fact too wide. Limiting the scope of declarations to a minimum reduces the probability of inadvertently using the variable, but it also improves code quality: the declaration of the variable is at the same location where the variable is first used, so the narrative is easier to follow. In C++ this may even have performance benefits since a declaration may trigger a call to a potentially expensive constructor. Fortran requires that variables are declared at the start of a compilation unit, i.e., program , function , subroutine , module , but Fortran 2008 introduced the block statement in which local variables can be declared. Their scope doesn't extend beyond the block . Modern compilers support this Fortran 2008 feature. Note that Fortran still allows variables to be implicitly typed, i.e., if you don't declare a variable explicitly, its type will be integer if its starts with the characters i to n , otherwise its type will be real . Consider the code fragment below. Since the variables were not declared explicitly, i is interpreted as integer and total as real . However, the misspelled totl is also implicitly typed as real , initialized to 0.0 , and hence the value of total will be 10.0 when the iterations ends, rather than 100.0 as was intended. integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do To avoid these problems caused by simple typos, use the implicit none statement before variable declarations in program , module , function , subroutine , and block , e.g, implicit none integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do The compiler would give an error for the code fragment above since all variables have to be declared explicitly, and totl was not. Limiting scope of of declarations extends to headers files that are included in C/C++. It is recommended not to include files that are not required. Not only will it pollute the namespace with clutter, but it will also increase build times. This advice is even more important for Python import statements. While the performance impact for C and C++ is limited to compile time, that of unnecessary imports of Python modules will increase the run time of your application. Multithreading When developing multi-threaded C/C++ programs using OpenMP, limiting the scope of variables to parallel regions makes those variables thread-private, hence reducing the risk of data races. We will discuss this in more detail in a later section. Unfortunately, the semantics for the Fortran block statement in an OpenMP do loop is not defined, at least up to the OpenMP 4.5 specification. Although gfortran accepts such code constructs, and seems to generate code with the expected behavior, it should be avoided since Intel Fortran compiler will report an error for such code. This recommendation is mentioned in the C++ core guidelines. Namespaces and imports In C++, you can importing everything defined in a namespace, e.g., using namespace std; Although it saves on typing, it is better to either use the namespace prefix explicitly, or use only what is required, e.g., using std::cout; using std::endl; In Fortran it is also possible to restrict what to use from modules, e.g., use, intrinsic :: iso_fortran_env, only : REAL64, INT32 The only keyword ensures that only the parameters REAL64 and INT32 are imported from the iso_fortran_env module. Note that the intrinsic keyword is used to ensure that the compiler supplied module is used, and not a module with the same name defined by you. Similar advice applies to Python, from math import * is considered bad practice since it pollutes the namespace. Be explicit about constants If a variable's value is not supposed to change during the run time of a program, declare it as a constant, so that the compiler will warn you if you inadvertently modify its value. In C/C++, use the const qualifier, in Fortran, use parameter . If arguments passed to function should be read-only, use const in C/C++ code, and intent(in) in Fortran. Although Fortran doesn't require that you state the intent of arguments passed to procedures, it is nevertheless wise to do so. The compiler will catch at least some programming mistakes if you do. However, this is not quite watertight, in fact, one can still change the value of a variable that is declared as a constant in C. Compile and run the following program, and see what happens. #include <stdio.h> void do_mischief(int *n) { *n = 42; } int main(void) { const int n = 5; printf(\"originally, n = %d\\n\", n); do_mischief((int *) &n); printf(\"mischief accomplished, n = %d\\n\", n); return 0; } In fact, this is explicitly mentioned in the C++ core guidelines. Control access When defining classes in C++ and Fortran, some attention should be paid to accessibility of object attributes. An object's state is determined by its attributes' values, so allowing unrestricted access to these attributes may leave the object in an inconsistent state. In C++, object attributes and methods are private by default, while structure fields and methods are public. For Fortran, fields in user defined types and procedures defined in modules are public by default. Regardless of the defaults, it is useful to specify the access restrictions explicitly. It is good practice to specify private access as the default, and public as the exception to that rule. Interestingly, both Fortran and C++ have the keyword protected , albeit with very different semantics. In Fortran, protected means that a variable defined in a module can be read by the compilation unit that uses it, but not modified. In the module where it is defined, it can be modified though. In C++, an attribute or a method that is declared protected can be accessed from derived classes as well as the class that defines it. However, like attributes and methods declared private , it can not be accessed elsewhere. This is another example where getting confused about the semantics can lead to interesting bugs. In summary: access modifier C++ Fortran private access restricted to class/struct access restricted to module protected access restricted to class/struct and derived variables: modify access restricted to module, read everywhere public attributes and methods can be accessed from everywhere variables, types and procedures can be accessed from everywhere none class: private, struct: public public Python has no notion of private attributes or methods, they are always public. However, attributes and methods that are supposed to be private to the class are by convention prefixed with a _ . Note that this is a convention for programmers, the Python runtime will not enforce this. In both C++ and Python you can \"simulate\" Fortran notion of protected , i.e., read-only attributes by implementing a getter, but no setter. Variable initialization The specifications for Fortran, C and C++ do not define the value an uninitialized variable will have. So you should always initialize variables explicitly, otherwise your code will have undefined, and potentially non-deterministic behavior. When you forget to initialize a variable, the compilers will typically let you get away with it. However, most compilers have optional flags that catch expressions involving uninitialized variables. We will discuss these and other compiler flags in a later section. When initializing or, more generally, assigning a value to a variable that involves constants, your code will be easier to understand when those values indicate the intended type. For example, using 1.0 rather than 1 for floating point is more explicit. This may also avoid needless conversions. This also prevents arithmetic bugs since 1/2 will evaluate to 0 in C, C++ as well as Fortran. Perhaps even more subtly, 1.25 + 1/2 will also evaluate to 1.25 , since the division will be computed using integer values, evaluating to 0 , which is subsequently converted to the floating point value 0.0 , and added to 1.25 . Specifically for C++, I'd strongly encourage you to use universal initialization, since narrowing conversion would lead to warnings. In the code fragment below, the first local variable n1 will be initialized to 7 without any warnings, while the compiler will generate a warning for the initialization of n2 . int conversion(double x) { int n1 = x; int n2 {x}; return n1 + n2; } Precision is also an important factor. If you intend to work purely in single precision for floating point arithmetic operations, it is important to avoid accidental type promotion. For instance, in the code below, the single precision argument is multiplied by 2.1 in ... double precision since that is what the compiler assumes you want to do. When that computation is done, the result is converted back to a single precision value. float times2(float x) { return 2.1*x; } In C and C++, a single precision floating value is denoted by 2.1f to indicate its type. float times2(float x) { return 2.1f*x; } In Fortran, you can similarly make the distinction between kinds of literal numerical values. function times2(x) result(y) implicit none use, intrinsic :: iso_fortran_env, only : sp => REAL32 real(kind=sp), intent(in) :: x real(kind=sp) :: y y = 2.1_sp*x return y end function Similar concerns are important when using numpy in Python. To comment or not to comment? Comments should never be a substitute for code that is easy to understand. In almost all circumstances, if your code requires a comment without which it can not be understood, it can be rewritten to be more clear. Obviously, there are exceptions to this rule. Sometimes we have no alternative but to sacrifice a clean coding style for performance, or we have to add an obscure line of code to prevent a problem caused by over-eager compilers. If you need to add a comment, remember that it should be kept up-to-date with the code. All too often, we come across comments that are no longer accurate because the code has evolved, but the corresponding comment didn't. In such situations, the comment is harmful, since it can confuse us about the intentions of the developer, and at the least, it will cost us time to disambiguate. The best strategy is to make sure that the code tells its own story, and requires no comments. A common abuse of comments is to disable code fragments that are no longer required, but that you still want to preserve. This is bad practice. Such comments make reading the code more difficult, and take up valuable screen real estate. Moreover, when you use a version control system such as git or subversion in your development process, you can delete with impunity, in the sure knowledge that you can easily retrieve previous versions of your files. If you don't use a version control system routinely, you really should. See the additional material section for some pointers to information and tutorials. You should also bear in mind the distinction between comments and documentation. Documentation describes how to use your data types and functions to those who may want to use them. Comments are intended for the consumption of the developers only. You can learn about best practices for documenting your code in the section on documentation . Stick to the standard The official syntax and semantics of languages like C, C++ and Fortran is defined in official specifications. All compilers that claim compliance with these standards have to implement these specifications. However, over the years, compiler developers have added extensions to the specifications. The Intel Fortran compiler for instance has a very long history that can trace its ancestry back to the DEC compiler, and implements quite a number of Fortran extensions. Similarly, the GCC C++ compiler supports some non-standard features. It goes without saying that your code should not rely on such compiler specific extensions, even if that compiler is mainstream and widely available. There is no guarantee that future releases of that same compiler will still support the extension, and the only official information about that extension would be available in the compiler documentation, not always the most convenient source. Moreover, that implies that even if your code compiles with a specific compiler, that doesn't mean it complies with the official language specification. An other compiler would simply generate error message for the same code, and would fail to compile it. Using language extensions makes code harder to read. As a proficient programmer, you're still not necessarily familiar with language extensions, so you may interpret those constructs incorrectly. Hence I'd encourage you strongly to strictly adhere to a specific language specification. For C there are four specifications that are still relevant, C89, C99, C11 and C23. For C++ that would be C++11, C++14, C++17, C++20 and C++23. The relevant specification for Fortran are those of 2003, 2008, and 2018. References to those specifications can be found in the reference section . For C and C++, you may be interested to read the MISRA software development guidelines, a collections of directives and rules specified by the Motor Industry Software Reliability Association (MISRA) aimed at ensuring safer and more reliable software systems in the automotive industry. A reference to this specification is mentioned in the [references][references.md]. The latest and greatest? Programming languages and libraries evolve over time. New features are added, some are deprecated. It is quite important to keep track of the evolution of the programming languages you use. New features are typically added to make your code more robust, or easier to read or write. Similarly, features are deprecated for a reason, usually because they were a Bad Idea(TM), or they can and should be replaced by new ones. In general, it is a good idea to keep up, i.e., start using new features, and especially replace code that is marked as deprecated by your compiler or interpreter. However, don't go over the top. If you use the very latest language features, e.g., the most recent version of Python, latest and greatest C++ standard you should bear in mind that you may cause problems for users of your application or library. It is quite possible that they use systems on which the latest compilers, interpreters and libraries are not available yet, sometimes for very good reasons. With software deployment in mind, you should try to strike a healthy balance between innovation and pragmatism. Maybe it is wiser not to use the latest and greatest feature just yet, but to leave it for the next release? Copy/paste is evil If you find yourself copying and pasting a fragment of code from one file location to another, or from one file to another, you should consider turning it into a function. Apart from making your code easier to understand, it makes it also easier to maintain. Suppose there is a bug in the fragment. If you copy/pasted it, you would have to remember to fix the bug in each instance of that code fragment. If it was encapsulated in a function, you would have to fix the problem in a single spot only. Reuse your code The warning on copy/paste is not only important in the context of a single project, but also across multiple projects. If you find yourself copying functions between projects, it is time to think to redesign your software in a more modular way to facilitate convenient code reuse. Each programming language offers mechanism to develop code in a modular way, e.g., header files in C/C++, modules in Fortran and Python, libraries in R. Structuring common data types and functions into reusable units makes it a lot easier to to reuse that code. In effect, you are building a library of your own. This library forms the core of many of your projects and can be maintained separately. Of course, this comes with a risk if you use your library in multiple projects, say both project A and project B. You have to take care that if you modify the Application Programming Interface (API) or the functionality for project A, it does not break project B. Again, many programming languages make it easy to maintain API compatibility, either through overloading in a language such as C++, or by creative use of optional arguments, allowed by Fortran and Python. For example, suppose you have a function to compute descriptive statistics. Currently, it computes the mean value. def statistics(data): return sum(data)/len(data) In a new project, you would like the function statistics not only to return the mean value, but also the number of data, so you could modify it as shown below. def statistics(data): n = len(data) return sum(data)/n, n Now the function returns a tuple , its first element the mean value, its second the number of elements in data. Although this would of course be fine for the new project, all other projects that rely on the function statistics would break since it would be expected that it returns a float value rather than a tuple . This can be avoided by adding an optional argument as follows. def statistics(data, return_n=False): n = len(data) mean = sum(data)/n return mean, n if return_n else mean Since the return_n argument defaults to False , the function will retain its previous behavior when called in all previous projects, while you can use the added functionality in the new project by calling the function as statistics(data, return_n=True) . Note that such design decisions need to be properly documented. Of course, this approach only works up to a point. It might be necessary to review and redesign the API once in a while to streamline it and reduce the cognitive burden. \"Not invented here\" syndrome Something I observe time after time is the \"not invented here\" syndrome. This means that a developer may prefer to do her own implementation of a function or a class rather than use an existing library developed by others. She may want to avoid the learning curve or the code changes required to use that library, or just doesn't trust a \"black box\". Sometimes that is a wise decision. The overhead incurred by using yet another library to accomplish something that is in fact quite simple to implement yourself may be too high. It will make dependency management and deployment somewhat more complicated . Although package managers are a great help in this respect, tooling is still required, and you may need to update your code if the API of the library changes. On the other hand, re-implementing functionality that already exists in third-party libraries is mostly not a good idea. You are reinventing the wheel, and you might end up with a square one. There may be bugs or performance issues, a lack of flexibility, and it incurs technical debt , i.e., you have to maintain that code for the rest of your software's life cycle. Again, you would have to try to find a healthy balance between implementing functionality yourself and using third-party libraries. Note that this may also have an impact on the license Follow the pattern About half a century into software development, it was recognized that good-quality software often exhibited the same patterns to implement certain features. The Gang of Four (Gamma, Helm, Johnson and Vlissides) published a book that cataloged the patterns they had found, put them into categories and gave both an abstract description and some concrete examples. This book has inspired many others on the same topic, sometimes geared to specific programming languages or application domains. It would lead us too far to go into details about design patters, but it is very useful to familiarize yourself with them so that you can apply them when the situation arises. You will find pointers in the reference section .","title":"Code style"},{"location":"code_style/#code-style","text":"A number of very simple things go a long way towards improving your code substantially. For good programmers, they are second nature, and you should strive to make them a habit. This text tries to be programming language-agnostic, and code fragments will be presented in several programming languages such as C, C++, Fortran and Python. However, even if you don't master these languages, the code fragments should be easy enough to understand. Although each programming languages has some specific best practices, many are applicable to any programming language. This is what this text focuses on. We also provide a list of references to best practices specific to various programming languages, and we encourage you strongly to read those as well. In this section, we will use the term function in a very broad sense, simply to keep the text easy to read. In the context of Fortran, a function refers to a program unit, any procedure, either a function or a subroutine. It also refers to a method defined in a class. In the context of Python and C++, we will use the term function for methods as well. Similarly, we use the term variable for constants, and also for attributes of objects and classes, whenever that doesn't lead to confusion. Of course, each programming language has its own style guides, often even several, you can find links to those in the reference section .","title":"Code style"},{"location":"code_style/#format-your-code-nicely","text":"To quote Robert C. Martin, \"Code formatting is about communication, and communication is the professional developer\u2019s first order of business\". All programming languages have one or possibly multiple conventions about how to format source code. For example, consistent indentation of code helps considerably to assess the structure of a function at a glance. For multi-level indentation, always use the same width, e.g., multiples of four spaces. The convention you have to use is often determined by the community you are working with, such as your co-workers. It is best to stick to that convention, since coding is communicating. If no convention is established, consider introducing one. The one which is prevalent in the programming language community is most likely to be your best choice. For several programming languages there are tools to automatically format your code so that it adheres to a convention. It is considered good practice to use such tools if they are available, and even to make them part of your development pipeline, either as git pre-commit hooks, or as part of a more substantial CI/CD setup on GitHub or GitLab. Another issue is code formatting is the maximum number of characters on a line. Some guidelines for programmers have very strong opinions on that, while other are more tolerant. Since many editors will automatically display a view on your code with lines wrapped to the length that can be displayed on the screen, that might not seem an issue, but it can be argued that this makes the code harder to read. If you switch that off, you will have a view on your code were some lines are truncated, again a very unpleasant and potentially confusing situation. Personally I like the maximum line length to be 80 characters, since that will guarantee it can be displayed without the need to wrap it, even when displaying multiple editor windows side-by-side. Again, code formatters can enforce this and properly wrap the code for you automatically. Whichever convention you follow, be consistent!","title":"Format your code nicely"},{"location":"code_style/#use-language-idioms","text":"Linguists use the term \"idiom\" for an expression that is very specific to a certain language and that cannot be translated literally to another. For instance, the English idiom \"it is raining cats and dogs\" would translate to \"il pleut des cordes\" in French. The corresponding idiom in French is completely unrelated to its counterpart in English. Mastering idioms is one of the requirements for C1 certification, i.e., to be considered to have a proficiency close to that of native speakers. We observe a similar phenomenon for programming languages. Some syntactic constructs are typical for a specific programming language but, when translated one-to-one into another language, lead to code constructs that are unfamiliar to programmers who are proficient in that language. The code fragments below illustrate this for Fortran and C. Although you could write line 4 of the C function below in this way, you most likely wouldn't since it is not idiomatic C. int factorial(int n) { fac = 1; for (int i = 2; i <= n; i++) fac = fac*i; return fac; } The idiomatic formulation of line 4 would be fac *= i . In Fortran for example, you would write real, dimension(10) :: a ... a = value rather than integer :: i real, dimension(10) :: a ... do i = 1, 10 a(i) = value end do Using idioms, i.e., expressions that are particular to a (programming) language, will make your code much easier to interpret correctly by programmers that are fluent in that language. In the spirit of Robert C. Martin's quote: it is all about communication. However, there are sometimes different reasons as well. If we consider the following Python code that uses the numpy library, we can observe a marked performance difference when comparing the two code fragments below. a = np.emptylike(b) for i in range(b.shape[0]): for j in range(b.shape[1]): a[i, j] = np.sqrt(b[i, j]) versus a = np.sqrt(b) The second form is more idiomatic, but it will also substantially outperform the first fragment. MATLAB users will be aware of this difference as well. Idioms for programming languages are usually expressed in code style guidelines. You can find links to those in the reference section .","title":"Use language idioms"},{"location":"code_style/#choose-descriptive-names","text":"In a way, programming is storytelling. The data are the protagonists in the story, and the functions are the actions they take, or what happens to them. Hence variable names should be nouns and functions names should be verbs. If a function returns a property, it should be phrased as a question. Any editor worth its salt provides completion, so you can't argue in favor of short but less descriptive names to save typing. A long but descriptive name is just hitting the tab key away. Choosing descriptive names for variables and functions is another aspect that can make reading your code much easier. Consider the following pseudo-code fragment, and although I'll grant that it is something of a caricature, I've seen some in the wild that are not significantly better. f = open(fn, 'r') for i in f: x = get(i) if condition(x): a = compute(x) if a < 3.14: do_something(a) f.close() A key principle of good software design is that of the least surprise . Choosing appropriate names for our variables and functions helps a lot in this respect. When it comes to programming languages, there is a very clear bias: all those I'm aware of have keywords defined in English. Hence I would argue that code that also uses English variable and function names is easier to read. This saves you from continuously, although unconsciously, to switch from one natural language to another. It may seem convenient, or even more natural to choose variable names in your native language, but that leads to more \"surprises\" when reading the code since you would also expect keywords in your native language to form actual sentences. Moreover, communicating about your code with others that don't speak your language will be just that bit harder. Just to put things in context: sometimes short variable names are perfectly adequate. For instance, trivial variable using in an iteration to index an array are conventionally called i , j and so on. This is perfectly fine, and unless you have a good reason to choose different names, the \"default\" names will even help others to understand your code. Also domain specific variables can have short names, using, e.g., T to denote the temperature, or t for time. This particular example would of course break for programming languages that are case-insensitive, e.g., Fortran. Note that these last remarks in no way contradicts the message of this section: T and t are very descriptive for developers in the domains where this notation is used.","title":"Choose descriptive names"},{"location":"code_style/#keep-it-simple","text":"Ideally, code is simple. A function should have two levels of indentation at most. This is advice you'll find in the literature on general purpose programming. Although this is good advice, there are some caveats in the context of scientific computing. However, the gist is clear: code is as simple as possible, but not simpler. Even for scientific code, a function has no more lines of code than fit comfortably on your screen. It is all too easy to lose track of the semantics if you can't get an overview of the code. Remember, not everyone has the budget for a 5K monitor. If you find yourself writing a very long code fragment, ask yourself whether that is atomic, or whether the task it represents can be broken up into sub-tasks. If so, and that is very likely, introduce new functions for those sub-tasks with descriptive names. This will make the narrative all the easier to understand. A function should have a single purpose, i.e., you should design it to do one thing, and one thing only. For function signatures, simplicity matters as well. Functions that take many arguments may lead to confusion. In C and C++, you have to remember the order of the function arguments. Accidentally swapping argument values with the same type in a function call can lead to interesting debugging sessions. The same advice applies to Fortran procedures or Python functions, keep the number of arguments limited. However, both Fortran and Python support using keyword arguments, a nice feature that makes your code more robust. Consider the following procedure signature: real function random_gaussian(mu, sigma) implicit none real, intent(in) :: mu, sigma ... end function random_gaussian You would have to check the documentation to know the order of the function arguments. Consider the following four function calls: random_gaussian(0.0, 1.0) : okay; random_gaussian(1.0, 0.0) : not okay; random_gaussian(mu=0.0, sigma=1.0) : okay; random_gaussian(sigma=1.0, mu=0.0) : okay. The two last versions of this call are easier to understand, since the meaning of the numbers is clear. Moreover, since you can use any order, it eliminates a source of bugs. Unfortunately, neither C nor C++ support this feature.","title":"Keep it simple"},{"location":"code_style/#limit-scope","text":"Many programmers will declare all variables at the start of a block, or even at the start of a function's implementation. This is a syntax requirement in C89 and Fortran. However, C99, C++, R and Python allow you to declare variables anywhere before their first use. Since the scope of a variable starts from its declaration, and extends throughout the block, that means it is in fact too wide. Limiting the scope of declarations to a minimum reduces the probability of inadvertently using the variable, but it also improves code quality: the declaration of the variable is at the same location where the variable is first used, so the narrative is easier to follow. In C++ this may even have performance benefits since a declaration may trigger a call to a potentially expensive constructor. Fortran requires that variables are declared at the start of a compilation unit, i.e., program , function , subroutine , module , but Fortran 2008 introduced the block statement in which local variables can be declared. Their scope doesn't extend beyond the block . Modern compilers support this Fortran 2008 feature. Note that Fortran still allows variables to be implicitly typed, i.e., if you don't declare a variable explicitly, its type will be integer if its starts with the characters i to n , otherwise its type will be real . Consider the code fragment below. Since the variables were not declared explicitly, i is interpreted as integer and total as real . However, the misspelled totl is also implicitly typed as real , initialized to 0.0 , and hence the value of total will be 10.0 when the iterations ends, rather than 100.0 as was intended. integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do To avoid these problems caused by simple typos, use the implicit none statement before variable declarations in program , module , function , subroutine , and block , e.g, implicit none integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do The compiler would give an error for the code fragment above since all variables have to be declared explicitly, and totl was not. Limiting scope of of declarations extends to headers files that are included in C/C++. It is recommended not to include files that are not required. Not only will it pollute the namespace with clutter, but it will also increase build times. This advice is even more important for Python import statements. While the performance impact for C and C++ is limited to compile time, that of unnecessary imports of Python modules will increase the run time of your application.","title":"Limit scope"},{"location":"code_style/#multithreading","text":"When developing multi-threaded C/C++ programs using OpenMP, limiting the scope of variables to parallel regions makes those variables thread-private, hence reducing the risk of data races. We will discuss this in more detail in a later section. Unfortunately, the semantics for the Fortran block statement in an OpenMP do loop is not defined, at least up to the OpenMP 4.5 specification. Although gfortran accepts such code constructs, and seems to generate code with the expected behavior, it should be avoided since Intel Fortran compiler will report an error for such code. This recommendation is mentioned in the C++ core guidelines.","title":"Multithreading"},{"location":"code_style/#namespaces-and-imports","text":"In C++, you can importing everything defined in a namespace, e.g., using namespace std; Although it saves on typing, it is better to either use the namespace prefix explicitly, or use only what is required, e.g., using std::cout; using std::endl; In Fortran it is also possible to restrict what to use from modules, e.g., use, intrinsic :: iso_fortran_env, only : REAL64, INT32 The only keyword ensures that only the parameters REAL64 and INT32 are imported from the iso_fortran_env module. Note that the intrinsic keyword is used to ensure that the compiler supplied module is used, and not a module with the same name defined by you. Similar advice applies to Python, from math import * is considered bad practice since it pollutes the namespace.","title":"Namespaces and imports"},{"location":"code_style/#be-explicit-about-constants","text":"If a variable's value is not supposed to change during the run time of a program, declare it as a constant, so that the compiler will warn you if you inadvertently modify its value. In C/C++, use the const qualifier, in Fortran, use parameter . If arguments passed to function should be read-only, use const in C/C++ code, and intent(in) in Fortran. Although Fortran doesn't require that you state the intent of arguments passed to procedures, it is nevertheless wise to do so. The compiler will catch at least some programming mistakes if you do. However, this is not quite watertight, in fact, one can still change the value of a variable that is declared as a constant in C. Compile and run the following program, and see what happens. #include <stdio.h> void do_mischief(int *n) { *n = 42; } int main(void) { const int n = 5; printf(\"originally, n = %d\\n\", n); do_mischief((int *) &n); printf(\"mischief accomplished, n = %d\\n\", n); return 0; } In fact, this is explicitly mentioned in the C++ core guidelines.","title":"Be explicit about constants"},{"location":"code_style/#control-access","text":"When defining classes in C++ and Fortran, some attention should be paid to accessibility of object attributes. An object's state is determined by its attributes' values, so allowing unrestricted access to these attributes may leave the object in an inconsistent state. In C++, object attributes and methods are private by default, while structure fields and methods are public. For Fortran, fields in user defined types and procedures defined in modules are public by default. Regardless of the defaults, it is useful to specify the access restrictions explicitly. It is good practice to specify private access as the default, and public as the exception to that rule. Interestingly, both Fortran and C++ have the keyword protected , albeit with very different semantics. In Fortran, protected means that a variable defined in a module can be read by the compilation unit that uses it, but not modified. In the module where it is defined, it can be modified though. In C++, an attribute or a method that is declared protected can be accessed from derived classes as well as the class that defines it. However, like attributes and methods declared private , it can not be accessed elsewhere. This is another example where getting confused about the semantics can lead to interesting bugs. In summary: access modifier C++ Fortran private access restricted to class/struct access restricted to module protected access restricted to class/struct and derived variables: modify access restricted to module, read everywhere public attributes and methods can be accessed from everywhere variables, types and procedures can be accessed from everywhere none class: private, struct: public public Python has no notion of private attributes or methods, they are always public. However, attributes and methods that are supposed to be private to the class are by convention prefixed with a _ . Note that this is a convention for programmers, the Python runtime will not enforce this. In both C++ and Python you can \"simulate\" Fortran notion of protected , i.e., read-only attributes by implementing a getter, but no setter.","title":"Control access"},{"location":"code_style/#variable-initialization","text":"The specifications for Fortran, C and C++ do not define the value an uninitialized variable will have. So you should always initialize variables explicitly, otherwise your code will have undefined, and potentially non-deterministic behavior. When you forget to initialize a variable, the compilers will typically let you get away with it. However, most compilers have optional flags that catch expressions involving uninitialized variables. We will discuss these and other compiler flags in a later section. When initializing or, more generally, assigning a value to a variable that involves constants, your code will be easier to understand when those values indicate the intended type. For example, using 1.0 rather than 1 for floating point is more explicit. This may also avoid needless conversions. This also prevents arithmetic bugs since 1/2 will evaluate to 0 in C, C++ as well as Fortran. Perhaps even more subtly, 1.25 + 1/2 will also evaluate to 1.25 , since the division will be computed using integer values, evaluating to 0 , which is subsequently converted to the floating point value 0.0 , and added to 1.25 . Specifically for C++, I'd strongly encourage you to use universal initialization, since narrowing conversion would lead to warnings. In the code fragment below, the first local variable n1 will be initialized to 7 without any warnings, while the compiler will generate a warning for the initialization of n2 . int conversion(double x) { int n1 = x; int n2 {x}; return n1 + n2; } Precision is also an important factor. If you intend to work purely in single precision for floating point arithmetic operations, it is important to avoid accidental type promotion. For instance, in the code below, the single precision argument is multiplied by 2.1 in ... double precision since that is what the compiler assumes you want to do. When that computation is done, the result is converted back to a single precision value. float times2(float x) { return 2.1*x; } In C and C++, a single precision floating value is denoted by 2.1f to indicate its type. float times2(float x) { return 2.1f*x; } In Fortran, you can similarly make the distinction between kinds of literal numerical values. function times2(x) result(y) implicit none use, intrinsic :: iso_fortran_env, only : sp => REAL32 real(kind=sp), intent(in) :: x real(kind=sp) :: y y = 2.1_sp*x return y end function Similar concerns are important when using numpy in Python.","title":"Variable initialization"},{"location":"code_style/#to-comment-or-not-to-comment","text":"Comments should never be a substitute for code that is easy to understand. In almost all circumstances, if your code requires a comment without which it can not be understood, it can be rewritten to be more clear. Obviously, there are exceptions to this rule. Sometimes we have no alternative but to sacrifice a clean coding style for performance, or we have to add an obscure line of code to prevent a problem caused by over-eager compilers. If you need to add a comment, remember that it should be kept up-to-date with the code. All too often, we come across comments that are no longer accurate because the code has evolved, but the corresponding comment didn't. In such situations, the comment is harmful, since it can confuse us about the intentions of the developer, and at the least, it will cost us time to disambiguate. The best strategy is to make sure that the code tells its own story, and requires no comments. A common abuse of comments is to disable code fragments that are no longer required, but that you still want to preserve. This is bad practice. Such comments make reading the code more difficult, and take up valuable screen real estate. Moreover, when you use a version control system such as git or subversion in your development process, you can delete with impunity, in the sure knowledge that you can easily retrieve previous versions of your files. If you don't use a version control system routinely, you really should. See the additional material section for some pointers to information and tutorials. You should also bear in mind the distinction between comments and documentation. Documentation describes how to use your data types and functions to those who may want to use them. Comments are intended for the consumption of the developers only. You can learn about best practices for documenting your code in the section on documentation .","title":"To comment or not to comment?"},{"location":"code_style/#stick-to-the-standard","text":"The official syntax and semantics of languages like C, C++ and Fortran is defined in official specifications. All compilers that claim compliance with these standards have to implement these specifications. However, over the years, compiler developers have added extensions to the specifications. The Intel Fortran compiler for instance has a very long history that can trace its ancestry back to the DEC compiler, and implements quite a number of Fortran extensions. Similarly, the GCC C++ compiler supports some non-standard features. It goes without saying that your code should not rely on such compiler specific extensions, even if that compiler is mainstream and widely available. There is no guarantee that future releases of that same compiler will still support the extension, and the only official information about that extension would be available in the compiler documentation, not always the most convenient source. Moreover, that implies that even if your code compiles with a specific compiler, that doesn't mean it complies with the official language specification. An other compiler would simply generate error message for the same code, and would fail to compile it. Using language extensions makes code harder to read. As a proficient programmer, you're still not necessarily familiar with language extensions, so you may interpret those constructs incorrectly. Hence I'd encourage you strongly to strictly adhere to a specific language specification. For C there are four specifications that are still relevant, C89, C99, C11 and C23. For C++ that would be C++11, C++14, C++17, C++20 and C++23. The relevant specification for Fortran are those of 2003, 2008, and 2018. References to those specifications can be found in the reference section . For C and C++, you may be interested to read the MISRA software development guidelines, a collections of directives and rules specified by the Motor Industry Software Reliability Association (MISRA) aimed at ensuring safer and more reliable software systems in the automotive industry. A reference to this specification is mentioned in the [references][references.md].","title":"Stick to the standard"},{"location":"code_style/#the-latest-and-greatest","text":"Programming languages and libraries evolve over time. New features are added, some are deprecated. It is quite important to keep track of the evolution of the programming languages you use. New features are typically added to make your code more robust, or easier to read or write. Similarly, features are deprecated for a reason, usually because they were a Bad Idea(TM), or they can and should be replaced by new ones. In general, it is a good idea to keep up, i.e., start using new features, and especially replace code that is marked as deprecated by your compiler or interpreter. However, don't go over the top. If you use the very latest language features, e.g., the most recent version of Python, latest and greatest C++ standard you should bear in mind that you may cause problems for users of your application or library. It is quite possible that they use systems on which the latest compilers, interpreters and libraries are not available yet, sometimes for very good reasons. With software deployment in mind, you should try to strike a healthy balance between innovation and pragmatism. Maybe it is wiser not to use the latest and greatest feature just yet, but to leave it for the next release?","title":"The latest and greatest?"},{"location":"code_style/#copypaste-is-evil","text":"If you find yourself copying and pasting a fragment of code from one file location to another, or from one file to another, you should consider turning it into a function. Apart from making your code easier to understand, it makes it also easier to maintain. Suppose there is a bug in the fragment. If you copy/pasted it, you would have to remember to fix the bug in each instance of that code fragment. If it was encapsulated in a function, you would have to fix the problem in a single spot only.","title":"Copy/paste is evil"},{"location":"code_style/#reuse-your-code","text":"The warning on copy/paste is not only important in the context of a single project, but also across multiple projects. If you find yourself copying functions between projects, it is time to think to redesign your software in a more modular way to facilitate convenient code reuse. Each programming language offers mechanism to develop code in a modular way, e.g., header files in C/C++, modules in Fortran and Python, libraries in R. Structuring common data types and functions into reusable units makes it a lot easier to to reuse that code. In effect, you are building a library of your own. This library forms the core of many of your projects and can be maintained separately. Of course, this comes with a risk if you use your library in multiple projects, say both project A and project B. You have to take care that if you modify the Application Programming Interface (API) or the functionality for project A, it does not break project B. Again, many programming languages make it easy to maintain API compatibility, either through overloading in a language such as C++, or by creative use of optional arguments, allowed by Fortran and Python. For example, suppose you have a function to compute descriptive statistics. Currently, it computes the mean value. def statistics(data): return sum(data)/len(data) In a new project, you would like the function statistics not only to return the mean value, but also the number of data, so you could modify it as shown below. def statistics(data): n = len(data) return sum(data)/n, n Now the function returns a tuple , its first element the mean value, its second the number of elements in data. Although this would of course be fine for the new project, all other projects that rely on the function statistics would break since it would be expected that it returns a float value rather than a tuple . This can be avoided by adding an optional argument as follows. def statistics(data, return_n=False): n = len(data) mean = sum(data)/n return mean, n if return_n else mean Since the return_n argument defaults to False , the function will retain its previous behavior when called in all previous projects, while you can use the added functionality in the new project by calling the function as statistics(data, return_n=True) . Note that such design decisions need to be properly documented. Of course, this approach only works up to a point. It might be necessary to review and redesign the API once in a while to streamline it and reduce the cognitive burden.","title":"Reuse your code"},{"location":"code_style/#not-invented-here-syndrome","text":"Something I observe time after time is the \"not invented here\" syndrome. This means that a developer may prefer to do her own implementation of a function or a class rather than use an existing library developed by others. She may want to avoid the learning curve or the code changes required to use that library, or just doesn't trust a \"black box\". Sometimes that is a wise decision. The overhead incurred by using yet another library to accomplish something that is in fact quite simple to implement yourself may be too high. It will make dependency management and deployment somewhat more complicated . Although package managers are a great help in this respect, tooling is still required, and you may need to update your code if the API of the library changes. On the other hand, re-implementing functionality that already exists in third-party libraries is mostly not a good idea. You are reinventing the wheel, and you might end up with a square one. There may be bugs or performance issues, a lack of flexibility, and it incurs technical debt , i.e., you have to maintain that code for the rest of your software's life cycle. Again, you would have to try to find a healthy balance between implementing functionality yourself and using third-party libraries. Note that this may also have an impact on the license","title":"\"Not invented here\" syndrome"},{"location":"code_style/#follow-the-pattern","text":"About half a century into software development, it was recognized that good-quality software often exhibited the same patterns to implement certain features. The Gang of Four (Gamma, Helm, Johnson and Vlissides) published a book that cataloged the patterns they had found, put them into categories and gave both an abstract description and some concrete examples. This book has inspired many others on the same topic, sometimes geared to specific programming languages or application domains. It would lead us too far to go into details about design patters, but it is very useful to familiarize yourself with them so that you can apply them when the situation arises. You will find pointers in the reference section .","title":"Follow the pattern"},{"location":"continuous_integration/","text":"Continuous integration Continuous Integration (CI) is provided by both GitHub and GitLab. It can be used to make sure that code that is committed is automatically tested and that it can be build, potentially on a matrix of architectures and operating systems. You can find an example of using CI for development in the repository CI-example . A workflow is defined that will run on a push or a pull request to both main and development . If that workflow fails, the pull request can not be merged. The workflow will run pytest and mypy to perform unit tests and static type analysis respectively. This can be a safeguard against accidentally merging commits that break your code. This also relies on the configuration of the main branch that requires the build to succeed in order to allow a merge. The same repository also illustrates how poetry can be used to manage Python development projects. You can also use CI to build your documentation using doxygen, mkdocs or other tools such as, e.g., Sphinx. The documentation can be automatically deployed using GitHub Pages thanks to predefined actions. In fact, these web pages are rendered an published using mkdocs and a GitHub workflow. This workflow will checkout the main branch, render the site using mkdocs in the gh-pages branch so that it is published. Each time a push is done to the main branch the workflow is run, and the latest version is guaranteed to be available via GitHub pages. You can find the workflow definition in the .github/workflows directory in the repository . Since this is in fact a deployment of documentation, it is referred to as Continuous Deployment (CD). It would of course also be possible to typeset a LaTeX document in a workflow, and make a PDF version available for download.","title":"Continuous integration"},{"location":"continuous_integration/#continuous-integration","text":"Continuous Integration (CI) is provided by both GitHub and GitLab. It can be used to make sure that code that is committed is automatically tested and that it can be build, potentially on a matrix of architectures and operating systems. You can find an example of using CI for development in the repository CI-example . A workflow is defined that will run on a push or a pull request to both main and development . If that workflow fails, the pull request can not be merged. The workflow will run pytest and mypy to perform unit tests and static type analysis respectively. This can be a safeguard against accidentally merging commits that break your code. This also relies on the configuration of the main branch that requires the build to succeed in order to allow a merge. The same repository also illustrates how poetry can be used to manage Python development projects. You can also use CI to build your documentation using doxygen, mkdocs or other tools such as, e.g., Sphinx. The documentation can be automatically deployed using GitHub Pages thanks to predefined actions. In fact, these web pages are rendered an published using mkdocs and a GitHub workflow. This workflow will checkout the main branch, render the site using mkdocs in the gh-pages branch so that it is published. Each time a push is done to the main branch the workflow is run, and the latest version is guaranteed to be available via GitHub pages. You can find the workflow definition in the .github/workflows directory in the repository . Since this is in fact a deployment of documentation, it is referred to as Continuous Deployment (CD). It would of course also be possible to typeset a LaTeX document in a workflow, and make a PDF version available for download.","title":"Continuous integration"},{"location":"deployment/","text":"Deployment Build tools Making sure you software can be built and installed easily is a very important part of the software's life cycle. It will be a hard requirement if you intend your software to be adopted and used by others. For compiled languages such as C, C++ and Fortran you will need to set up how to compile and link your code. Several build environment are available such as autotools and CMake. Although scripting languages such as Python and R don't require compilation you would still rely on these environments if you build extensions for these languages. Although it takes time to properly configure the build process, and there is a learning curve, it will save you a lot of time in the long run. Most build environments also allow you to run tests, and that includes those for scripting languages. Integrating test execution as part of your build process is most certainly a best practice. Deployment Most build environments let you easily create distributions of your software projects, for instance under the form of source archives that include the build configuration files. Some environments go even a step further by supporting uploads to software distribution repositories. The build systems can also be configured to do the installation of software, something that can be tuned further by the person who installs the software on her own system. Package managers Many software project rely on packages that need to be installed to successfully link or run the application or library. Managing these dependencies can be pretty tedious. Some build tools will help you to easily integrate such libraries by configuring the build configuration with the information on where they are installed on a system. Most build systems also support a form a dependency management or package management that allows you to easily download and install packages. Typically these installs will be local to the project, so it is not necessary to install them centrally and \"pollute\" your system. This dependency isolation makes it easier to test the software build system and makes it more portable to other computers and operating systems. License A very important aspect of software deployment is licensing. Releasing software without a license or with one that is not appropriate can have dire consequences. It is also very important to verify that the licenses of software component or tools are compatible with the one you select for yourself. There is a nice website that helps you select an appropriate license . However, when in doubt, get in touch with your Technology Transfer Office, they will be able to advice. Can I cite you on that? If you choose a license that enforces those that use it to give you credit, it makes sense to make it as easy as possible for them to do so. The same applies if you hope to be cited in scientific publications that use your software. You can associate a Digital Object Identifier (DOI) with your GitHub repository. This service is offered by Zenodo that integrates well with GitHub. When you do a release, your repository gets a DOI that you can include in your README.md as a badge. This is the case for the GitHub repository for this training material. Examples Examples of using various tools can be found in other repositories. CMake use cases Poetry C/C++ package managers","title":"Deployment"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"deployment/#build-tools","text":"Making sure you software can be built and installed easily is a very important part of the software's life cycle. It will be a hard requirement if you intend your software to be adopted and used by others. For compiled languages such as C, C++ and Fortran you will need to set up how to compile and link your code. Several build environment are available such as autotools and CMake. Although scripting languages such as Python and R don't require compilation you would still rely on these environments if you build extensions for these languages. Although it takes time to properly configure the build process, and there is a learning curve, it will save you a lot of time in the long run. Most build environments also allow you to run tests, and that includes those for scripting languages. Integrating test execution as part of your build process is most certainly a best practice.","title":"Build tools"},{"location":"deployment/#deployment_1","text":"Most build environments let you easily create distributions of your software projects, for instance under the form of source archives that include the build configuration files. Some environments go even a step further by supporting uploads to software distribution repositories. The build systems can also be configured to do the installation of software, something that can be tuned further by the person who installs the software on her own system.","title":"Deployment"},{"location":"deployment/#package-managers","text":"Many software project rely on packages that need to be installed to successfully link or run the application or library. Managing these dependencies can be pretty tedious. Some build tools will help you to easily integrate such libraries by configuring the build configuration with the information on where they are installed on a system. Most build systems also support a form a dependency management or package management that allows you to easily download and install packages. Typically these installs will be local to the project, so it is not necessary to install them centrally and \"pollute\" your system. This dependency isolation makes it easier to test the software build system and makes it more portable to other computers and operating systems.","title":"Package managers"},{"location":"deployment/#license","text":"A very important aspect of software deployment is licensing. Releasing software without a license or with one that is not appropriate can have dire consequences. It is also very important to verify that the licenses of software component or tools are compatible with the one you select for yourself. There is a nice website that helps you select an appropriate license . However, when in doubt, get in touch with your Technology Transfer Office, they will be able to advice.","title":"License"},{"location":"deployment/#can-i-cite-you-on-that","text":"If you choose a license that enforces those that use it to give you credit, it makes sense to make it as easy as possible for them to do so. The same applies if you hope to be cited in scientific publications that use your software. You can associate a Digital Object Identifier (DOI) with your GitHub repository. This service is offered by Zenodo that integrates well with GitHub. When you do a release, your repository gets a DOI that you can include in your README.md as a badge. This is the case for the GitHub repository for this training material.","title":"Can I cite you on that?"},{"location":"deployment/#examples","text":"Examples of using various tools can be found in other repositories. CMake use cases Poetry C/C++ package managers","title":"Examples"},{"location":"documentation/","text":"Documentation best practices When you consider using a new software library, you probably like to start by looking at some example code. You'll do a few experiments of your own, based on those examples. Once you start using the new library for non-trivial applications, you will most likely have to refer to the reference guide. All this documentation is a great help if its quality is good. If you want to deliver high quality software, the documentation is a very important aspect of the development process. You can distinguish between two types of documentation: tutorial style and reference documentation. Types of documentation Broadly speaking, we distinguish two types of documentation, tutorials and reference guides. They serve different purposes, and hence are complementary. Tutorials will typically illustrate a number of use cases, presented as a narrative. It is more of a high-level overview of the software project. Such documentation is typically written separately from the source code, and a convenient format is markdown. We will discuss MkDocs, a tools to generate nice looking documentation based on this format. A reference guide serves a different purpose. It is intended to get low-level information on the contents of the library. It should be easy to navigate, so that one can view the definition of a datatype by clicking it when it occurs in the signature of a function. Hence this type of documentation integrates closely with the source code itself. If the latter changes, so should the former. Pitfalls When working on a software project, one of the main pitfalls is neglecting documentation. Many developers don't particularly enjoy writing documentation, and hence postpone it. The more classes and functions to be documented, the worse the tasks seems to be, and the larger the probability that it will never happen. Getting into the habit of immediately writing the reference documentation for each module, data type or function as you code is certainly a \"best practice\". Although it may seem to slow you down while coding, that is not necessarily a bad thing. However, even when documentation is diligently written while coding, it has to be kept up to date when code changes. It is all too easy to forget to update the documentation when the signature of a function is modified. Inaccurate documentation is almost worse than no documentation since it is likely to cause serious software defects. What to document? The first question to answer is, what do you need to document since a software project has many artifacts. Documenting functions Consider the documentation of a function or a subroutine for instance. You would write a description of what the it does, which argument it takes, and what the return type is. However, you would also add the assumptions the procedure makes about the values its arguments can have. Such assumptions are called preconditions. By way of example, consider a function that takes a temperature as an argument. An important aspect is the units it expects the temperature to be specified in. Will that be degrees Celsius, or Fahrenheit, or Kelvin? In case it is degrees Celsius, it doesn't make sense to pass a value to the function that is less than -273.15 degrees Celsius. On the other hand, if it is Kelvin, the value should be positive. Needless to say that if you pass a temperature expressed in Kelvin to a function that expects degrees Celsius as a unit, the results will be wrong. This example illustrates an important point: the function documentation should clearly specify its expectation about the arguments that you pass to it. In our example, that would be the units of the temperature, but also the valid range of values. These restrictions are often called preconditions, and they constitute a contract between the user of the procedure and its author. The contract, i.e., the precondition, says that if the user of the function supplies arguments that fulfill all the preconditions, then the author guarantees that the function will return proper results. When considering values returned by a function, you have to consider similar restrictions. What are the units of the return value and what is the range of values that would make sense? This amounts to postconditions, again a contract between the function's author and its user. The contract, i.e., the postcondition, specifies that the user of the function can rely on it to return a value that fulfills all the conditions if she calls it correctly. When a procedure has side effects, i.e., it modifies one of its arguments, this should be stated as well. The \"principle of least astonishment\" is very useful in software development. Conversely, when passing an argument to a function that is not changed at all, that constitutes an invariant. Although it may be useful to state this in the documentation, it is far better practice to make that explicit by declaring it constant. The concepts of preconditions, postconditions, and invariants were introduced in the \"design by contract\" paradigm, pioneered by Bertrand Meyer in 1986. If you make it part of your coding process to immediately provide the documentation when writing a procedure, it will actually help you since you have to make your assumptions explicit, and hence may discover potential flaws. Documenting failure is very important as well. Can the function generate an error? What is the semantics of these errors? Again, this is an application of the \"principle of least astonishment\". The user of your functions will be aware of potential failure, and can code to deal with it appropriately. Documenting data types For data types, the semantics of the type should be documented. What does a variable of the type represent? Of course, the type name should be chosen so that this is clear, but it doesn't hurt to explain this more formally in the documentation as well. For each field, you should document its type, semantics, and, if applicable, the units that are expected. When doing object oriented programming, the documentation of methods is similar to that of functions. Documenting modules Code is typically aggregated according to functionality in modules for separate compilation or importing. Hence this aggregation has a purpose, and you should document that as well. What is the overall purpose of the module? Another question that you can answer in module documentation is the interaction between various components. How can the output of one function be used as the input for another, are the various functions independent, or should they be called in a certain order? As an example, if you have a function that initializes a data structure, a few that manipulate such data structures, and a finalization function that releases the resources in that data structure, then the module documentation should probably mention that you should call the initialization function to get a valid data structure; call functions that use the data structure; and call the finalization function to avoid memory leaks. At this level, you may want to add some example code of using the software components in that module. How do you call the procedures, how can you use the return values, what are the use cases? This type of documentation could also be maintained at the project level, though. Documenting projects This is typically high-level documentation in the form of a tutorial with use cases and code samples. It should of course contain a link to the reference documentation.","title":"Documentation"},{"location":"documentation/#documentation-best-practices","text":"When you consider using a new software library, you probably like to start by looking at some example code. You'll do a few experiments of your own, based on those examples. Once you start using the new library for non-trivial applications, you will most likely have to refer to the reference guide. All this documentation is a great help if its quality is good. If you want to deliver high quality software, the documentation is a very important aspect of the development process. You can distinguish between two types of documentation: tutorial style and reference documentation.","title":"Documentation best practices"},{"location":"documentation/#types-of-documentation","text":"Broadly speaking, we distinguish two types of documentation, tutorials and reference guides. They serve different purposes, and hence are complementary. Tutorials will typically illustrate a number of use cases, presented as a narrative. It is more of a high-level overview of the software project. Such documentation is typically written separately from the source code, and a convenient format is markdown. We will discuss MkDocs, a tools to generate nice looking documentation based on this format. A reference guide serves a different purpose. It is intended to get low-level information on the contents of the library. It should be easy to navigate, so that one can view the definition of a datatype by clicking it when it occurs in the signature of a function. Hence this type of documentation integrates closely with the source code itself. If the latter changes, so should the former.","title":"Types of documentation"},{"location":"documentation/#pitfalls","text":"When working on a software project, one of the main pitfalls is neglecting documentation. Many developers don't particularly enjoy writing documentation, and hence postpone it. The more classes and functions to be documented, the worse the tasks seems to be, and the larger the probability that it will never happen. Getting into the habit of immediately writing the reference documentation for each module, data type or function as you code is certainly a \"best practice\". Although it may seem to slow you down while coding, that is not necessarily a bad thing. However, even when documentation is diligently written while coding, it has to be kept up to date when code changes. It is all too easy to forget to update the documentation when the signature of a function is modified. Inaccurate documentation is almost worse than no documentation since it is likely to cause serious software defects.","title":"Pitfalls"},{"location":"documentation/#what-to-document","text":"The first question to answer is, what do you need to document since a software project has many artifacts.","title":"What to document?"},{"location":"documentation/#documenting-functions","text":"Consider the documentation of a function or a subroutine for instance. You would write a description of what the it does, which argument it takes, and what the return type is. However, you would also add the assumptions the procedure makes about the values its arguments can have. Such assumptions are called preconditions. By way of example, consider a function that takes a temperature as an argument. An important aspect is the units it expects the temperature to be specified in. Will that be degrees Celsius, or Fahrenheit, or Kelvin? In case it is degrees Celsius, it doesn't make sense to pass a value to the function that is less than -273.15 degrees Celsius. On the other hand, if it is Kelvin, the value should be positive. Needless to say that if you pass a temperature expressed in Kelvin to a function that expects degrees Celsius as a unit, the results will be wrong. This example illustrates an important point: the function documentation should clearly specify its expectation about the arguments that you pass to it. In our example, that would be the units of the temperature, but also the valid range of values. These restrictions are often called preconditions, and they constitute a contract between the user of the procedure and its author. The contract, i.e., the precondition, says that if the user of the function supplies arguments that fulfill all the preconditions, then the author guarantees that the function will return proper results. When considering values returned by a function, you have to consider similar restrictions. What are the units of the return value and what is the range of values that would make sense? This amounts to postconditions, again a contract between the function's author and its user. The contract, i.e., the postcondition, specifies that the user of the function can rely on it to return a value that fulfills all the conditions if she calls it correctly. When a procedure has side effects, i.e., it modifies one of its arguments, this should be stated as well. The \"principle of least astonishment\" is very useful in software development. Conversely, when passing an argument to a function that is not changed at all, that constitutes an invariant. Although it may be useful to state this in the documentation, it is far better practice to make that explicit by declaring it constant. The concepts of preconditions, postconditions, and invariants were introduced in the \"design by contract\" paradigm, pioneered by Bertrand Meyer in 1986. If you make it part of your coding process to immediately provide the documentation when writing a procedure, it will actually help you since you have to make your assumptions explicit, and hence may discover potential flaws. Documenting failure is very important as well. Can the function generate an error? What is the semantics of these errors? Again, this is an application of the \"principle of least astonishment\". The user of your functions will be aware of potential failure, and can code to deal with it appropriately.","title":"Documenting functions"},{"location":"documentation/#documenting-data-types","text":"For data types, the semantics of the type should be documented. What does a variable of the type represent? Of course, the type name should be chosen so that this is clear, but it doesn't hurt to explain this more formally in the documentation as well. For each field, you should document its type, semantics, and, if applicable, the units that are expected. When doing object oriented programming, the documentation of methods is similar to that of functions.","title":"Documenting data types"},{"location":"documentation/#documenting-modules","text":"Code is typically aggregated according to functionality in modules for separate compilation or importing. Hence this aggregation has a purpose, and you should document that as well. What is the overall purpose of the module? Another question that you can answer in module documentation is the interaction between various components. How can the output of one function be used as the input for another, are the various functions independent, or should they be called in a certain order? As an example, if you have a function that initializes a data structure, a few that manipulate such data structures, and a finalization function that releases the resources in that data structure, then the module documentation should probably mention that you should call the initialization function to get a valid data structure; call functions that use the data structure; and call the finalization function to avoid memory leaks. At this level, you may want to add some example code of using the software components in that module. How do you call the procedures, how can you use the return values, what are the use cases? This type of documentation could also be maintained at the project level, though.","title":"Documenting modules"},{"location":"documentation/#documenting-projects","text":"This is typically high-level documentation in the form of a tutorial with use cases and code samples. It should of course contain a link to the reference documentation.","title":"Documenting projects"},{"location":"license/","text":"Attribution 4.0 International ======================================================================= Creative Commons Corporation (\"Creative Commons\") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \"as-is\" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible. Using Creative Commons Public Licenses Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses. Considerations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC- licensed material, or material used under an exception or limitation to copyright. More considerations for licensors: wiki.creativecommons.org/Considerations_for_licensors Considerations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor's permission is not necessary for any reason--for example, because of any applicable exception or limitation to copyright--then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More_considerations for the public: wiki.creativecommons.org/Considerations_for_licensees ======================================================================= Creative Commons Attribution 4.0 International Public License By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions. Section 1 -- Definitions. a. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image. b. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License. c. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights. d. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements. e. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material. f. Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License. g. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license. h. Licensor means the individual(s) or entity(ies) granting rights under this Public License. i. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them. j. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world. k. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning. Section 2 -- Scope. a. License grant. 1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to: a. reproduce and Share the Licensed Material, in whole or in part; and b. produce, reproduce, and Share Adapted Material. 2. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions. 3. Term. The term of this Public License is specified in Section 6(a). 4. Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a) (4) never produces Adapted Material. 5. Downstream recipients. a. Offer from the Licensor -- Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License. b. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material. 6. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i). b. Other rights. 1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise. 2. Patent and trademark rights are not licensed under this Public License. 3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties. Section 3 -- License Conditions. Your exercise of the Licensed Rights is expressly made subject to the following conditions. a. Attribution. 1. If You Share the Licensed Material (including in modified form), You must: a. retain the following if it is supplied by the Licensor with the Licensed Material: i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated); ii. a copyright notice; iii. a notice that refers to this Public License; iv. a notice that refers to the disclaimer of warranties; v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable; b. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and c. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License. 2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information. 3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable. 4. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License. Section 4 -- Sui Generis Database Rights. Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material: a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database; b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database. For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights. Section 5 -- Disclaimer of Warranties and Limitation of Liability. a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU. b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU. c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability. Section 6 -- Term and Termination. a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically. b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates: 1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or 2. upon express reinstatement by the Licensor. For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License. c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License. d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License. Section 7 -- Other Terms and Conditions. a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed. b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License. Section 8 -- Interpretation. a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License. b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions. c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor. d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority. ======================================================================= Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark \"Creative Commons\" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses. Creative Commons may be contacted at creativecommons.org.","title":"License"},{"location":"optimization/","text":"Optimization In scientific computing, we potentially care a lot about the performance of our workflows or applications. We want to study larger systems, more complex models, or consider more variations of our problem. This often means we need to optimize our code to run faster, use less memory, or scale to more processors. In this section, a few general strategies for optimization are discussed. It is not really possible to go into the details since optimization is often very problem specific. Benchmarking Before you start optimizing your workflow or application, you should establish a baseline. This means you should measure the performance of the current version of your workflow or application. This will give you a reference point to compare against. The benchmark should be chosen so that it is representative of the workload you are interested in. For example, it will likely not be useful to measure the performance of a workflow on a small data set if you intend to run it on a large data set. The memory usage and the runtime of the workflow will likely be very different, and hence the benchmark will not be representative. You should also make sure that you can run the benchmark easily and consistently. This means that you should automate the benchmarking process as much as possible. This will allow you to run the benchmark often and to compare the results. You should also make sure that you can reproduce the benchmark. This means that you should document the benchmark and the environment in which it was run. This will allow you to compare the results of the benchmark over time and to compare the results of the benchmark on different systems. You will find more information on this topic in the section on reproducibility . Testing Having tests in place is vital before starting to optimize your workflow or application. You will want to make sure that everything still works after you make changes. You will find more information on this topic in the section on testing . Software stack If you are using third party software, make sure you are using the version that is optimized for your system. This means that the software is compiled with the right compiler, with the right flags, and with the right libraries. Some software environments such as R will always build additional R packages from source. If you use a version of R built specifically for your system, the packages will also be built for your system. This can make a big difference in performance. For Python, you can use the Intel distribution of Python which is optimized for Intel processors. Intel also provides optimized versions of numpy, scipy, and many machine learning-related packages. When installing software from source, make sure to use the right compiler and the right flags. This can make a big difference in performance. See the section on compilers and their flags for more information. If you work on an HPC system, it is very likely that the software stack provided by the module system is already optimized for the system. Make sure to use the modules if you can. Even if several libraries offer the same functionality, they may not be equally efficient. For example, the Intel Math Kernel Library (MKL) is sometimes faster than OpenBLAS/LAPACK. Another example is the use of the Intel MPI library instead of Open MPI. The Intel MPI library is often faster than Open MPI, especially on Intel processors. However, you should experiment, since for some codes, Open MPI may be faster. Your own code If the bulk of the time your workflow takes is spent in your own code, you you should consider optimizing your code. This can be done in many ways, use a faster compiler and compiler flags to optimize your code; a profiler to identify bottlenecks in your code; a faster algorithm and/or a more efficient data structure; a faster language; vectorization to speed up your code; parallelism to speed up your code. Compilers and their flags For Fortran, C and C++, make sure you are using the right compiler and the right flags. For example, the Intel compiler can often generate faster code than the GNU compiler. However, you should experiment, since for some codes, the GNU compiler may generate faster code. Make sure to compile with optimization flags such as -O3 and -xHost (Intel) or -march=native (GNU). Many other compiler flag influence performance, but that is outside the scope of this section. When using CMake, make sure to set the build type to Release to enable optimizations, i.e., cmake -DCMAKE_BUILD_TYPE=Release .. . CMake will then pass the appropriate flags to the compiler(s). Profiling Your first priority is to identify the bottlenecks in your code. This can be done with a profiler. A profiler will tell you where your code spends most of its time. You can then focus on optimizing these parts of your code. You may think that you know where the bottlenecks are, but you may be surprised by the results of a profiler. It is always a good idea to profile your code before you start optimizing it. To make this concrete, suppose that your intuition tells you that you should improve the performance of a function, and you spend two days working on it, and are rewarded by a very impressive improvement by a factor of 10. This was excellent work. However, if your application spends only 5 % of its time in this function, what will be the runtime of the new version of your application if that is the only thing you change? If the total runtime of the original application is \\(t\\) , it means that its runtime after your optimization will be \\(0.95t + 0.5t/10 = 0.955t\\) , i.e., you improved the overall runtime by less than 5 %. In some circumstances this may be worth the effort you spent, but if you would have profiled your application, you might have chosen to spend your time differently. Remember that you should profile in representative circumstances, see the section on benchmarking for a more thorough discussion. There are many tools to profile your application, some open source, but commercial as well. Some tools can profile parallel applications. See the tools section for more information. Algorithms and data structures The choice of the algorithm you use to solve a particular problem, or the data structures you use to represent your data can have a profound impact on the performance of your application. To illustrate this, consider a very simple example, sorting a list of numbers. There are many algorithms to do this, and all result in the same sorted list. However, the choice of the algorithm will depend on the length of the list. A very simple algorithm is bubble sort which on average implies \\(O(N^2)\\) comparison operations where \\(N\\) is the length of the list. However, the quicksort algorithm that is a bit more sophisticated will on average only have to perform \\(O(N \\log N)\\) comparisons. For long lists, this makes a huge difference. For example, if the list has a 1,000 elements ( \\(N = 1000\\) ), then bubble sort will take a million comparisons, while quicksort will only require 3,000, and hence is a factor of 300 faster. The choice of data structures you make can also have a profound impact on performance. If the goal of the data structure is to hold, e.g., numbers to allow to check whether or not a number occurred previously, you can use a list or a set. Checking membership in a list will take \\(O(N)\\) comparisons on average, while checking membership of a set only takes \\(O(1)\\) (ideally). Just like the choice of algorithm in the previous example, this may have an important impact on performance. This section only draws your attention to the problem, it would be impossible to go into more detail, but it is a very good investment of your time to familiarize yourself with the subject. Many excellent books have been published on the subject, covering the most common algorithms, and a lot of articles have been published that address very specific algorithms and specialized data structures. Programming languages Holy wars are waged on the subject of programming languages. Many of the discussions royally miss the point completely. In general compiled languages such as Fortran, C and C++ will produce faster applications. However, it is typically also more difficult to write applications in such a language then it is in R or Python. This implies that if your application is not performance critical, it would be a waste of time to write it in Fortran, C, or C++. Your time as a researcher is also quite valuable, so a short time to solution will be appreciated by the taxpayer. A good compromise is to implement the performance-critical parts of the application in a language such as Fortran, C, or C++, and wrap the resulting shared libraries so that they can be used from Python or R. This gives you the best of both worlds. Often, the work is already done for you. Consider the somewhat extreme example of machine learning. Frameworks such as TensorFlow or PyTorch allow you to write your code in Python at a high lever, while relying on libraries that were developed by HPC specialists under the hood. The execution time spent in pure Python code is completely negligible when compared to that spent on computations done by these core libraries. Again, profiling is crucial to determine what parts of your application make good candidates for reimplementation in a more performant programming language. Julia takes a somewhat different approach. The code you write will under the hood be translated to machine code before it is actually executed. Hence you get quite good performance out of the box when using Julia (well). Vectorization In scientific computing, core computations often consists of loops over arrays. If these loops implement matrix-vector or matrix-matrix operations, it is likely not a good idea to write such code. Consider using a library to represent these data structures that uses a BLAS (Basic Linear Algebra Subprograms) or LAPACK (Linear Algebra PACKage) library under the hood. These libraries have been developed with vectorization in mind, i.e., multiple iterations of the loops required for matrix-vector or matrix-matrix multiplication are done in parallel using wide registers. Depending on the numerical precision (single or double), the compiler flags used, as well as the hardware you compiled your code for that may range from 2 to 16. Since such operations are often at the core of your computations, ensuring that you benefit from vectorization may give you a significant performance benefit. In your own code, the compiler can often automatically apply vectorization on your own loops, given that it can prove that the iterations are independent of one another. To ensure that the compiler will attempt this, you would have to use the appropriate compiler flags. For the GNU compiler, this would be -ftree-vectorize , and -ftree-vectorizer-verbose=2 to see what the compiler did. For the Intel compiler, this would be -qopt-report=2 to see what the compiler did. Sometimes the compiler can not prove that vectorization is possible, i.e., that the iterations are independent. If you as a programmer knows for sure that although loop iterations are dependent, that dependence is on iterations that are not within the vector length, you can help the compiler by providing the appropriate OpenMP SIMD (Single Instruction, Multiple Data) directives. Again, this exceeds the scope of this section and you are referred to trainings on this specific topic. Parallelization Parallelization comes in many forms at various levels and complexity. Embarrassingly parallel The most obvious and common situation is that you have to run your workflow or application on many different data sets, or with a variety of (hyper)parameter values. This form of parallelism is called \"embarrassingly parallel\" because it is very simple to implement. It is as simple as running your workflow or application on as many compute resources as you can get. The CPU (Central Processing Unit) of a modern laptop has multiple cores, each core capable of executing an application independent and in parallel with applications running on other cores. A simple shell script using GNU parallel can help you to run such workloads easily on your own system. On HPC clusters, more specialized tools exist that serve the same purpose, using the scheduler to run your workload in parallel. Some examples of such tools are atools and worker-ng . If your use case doesn't match this approach, or individual runs of your workflow or application require too much time then other options are available at the price of (often) much higher complexity and effort on your part. Shared memory programming As mentioned before, modern CPUs have multiple cores, today ranging for typically 4 in your laptop to 128 in HPC compute nodes. Shared memory programming allows your application to utilize (potentially) all of these cores to perform computations in parallel, hence speeding up your application. Again, using the right libraries may give you a free lunch, similar to vectorization. Intel MKL and OpenBLAS/LAPACK will execute operations on multiple course if you link to the correct implementation. This is also the case for other libraries that use these under the hood. For example, the Armadillo C++ library will use BLAS and LAPACK libraries if available. The same applies to Python and R packages, e.g., numpy can use a BLAS library under the hood to perform matrix operations, hence using multiple cores for the computations. For Fortran, C, and C++, OpenMP is a good option to parallelize your code in a shared memory context. Typically, you annotate your source code with directives on, e.g., loop constructs to indicate to the compiler that these loops should be parallelized. The compiler will generate the appropriate instructions for you. This sounds suspiciously easy, and indeed, it is not so simple in practice to obtain good efficiency. Specifically for C++ a few more options are available. Many algorithms in the STL (Standard Template Library) can be executed in parallel with minimal changes to your source code. Of course, this is restrictions to those algorithms that support it. An alternative to OpenMP for C++ is TBB (Threading Building Blocks). This library is purely task oriented, with an excellent task scheduler under the hood. For Python, packages such as Cython and Numba will allow you to leverage multiple cores explicitly or implicitly, respectively. Again, you are referred to specific trainings on this subject. GPGPUs If your compute system is equiped with a graphics card that has compute capabilities (certain NVIDIA or AMD GPUs) then this is another level of parallelism that can be exploited. Again, certain libraries will do this for you under the hood, e.g., machine learning frameworks such as TensorFlow and PyTorch . More general frameworks such as cupy and Numba allow you to offload more general computations to the GPU. For Fortran, C, and C++ OpenACC or OpenMP may be an excellent solutions since compilers that implement these standards will generate appropriate code to transfer data to and from the GPU device, and run kernels. For C and C++ vendors of GPUs offer specific programming language extensions that work only on the specific hardware. This is CUDA for NVIDIA GPUs, and HIP for AMD hardware. This is a trade-off between (potentially) better performance and vendor lock-in. Specifically for C++ there are even more options: Data Parallel C++ (based on SYCL, developed by Intel) and the Kokkos library. Getting good performance on GPUs is not a trivial task due to data transport between host and device memory. Also, not all types of algorithms map well on the hardware architecture of such devices. Using multiple computers Up to this point, parallelization was limited to the possibilities a single system offered, multiple cores and perhaps one or more GPUs. To further scale and use multiple compute nodes more work is required. Relatively simple option for some tasks exist for Python. Frameworks like Dask or PySpark allow you to distributes computations over multiple compute nodes. Note that these are not general solutions, but will support certain use cases only. If your problem fits one of those use cases, substantial gains can be made without too much effort. For Python, Fortran, C, and C++ Message Passing Interface (MPI) may be an option. This is a standard implemented in various libraries such as Intel MPI, Open MPI, MVAPICH and others. Using such a library requires a substantial rewrite of your code. The standard offers great flexibility and the potential for excellent parallel performance and scaling to a large number of nodes. To similarly parallelize code on multiple NVIDIA GPUs on multiple compute nodes, NCCL can be used. Conceptually, it is quite similar to MPI, but messages are exchanged between processes running on GPUs, rather than processes on the CPUs. Finally, Data Parallel C++ and Kokkos can also be used from C++ to parallelize your code over multiple nodes using MPI.","title":"Optimization"},{"location":"optimization/#optimization","text":"In scientific computing, we potentially care a lot about the performance of our workflows or applications. We want to study larger systems, more complex models, or consider more variations of our problem. This often means we need to optimize our code to run faster, use less memory, or scale to more processors. In this section, a few general strategies for optimization are discussed. It is not really possible to go into the details since optimization is often very problem specific.","title":"Optimization"},{"location":"optimization/#benchmarking","text":"Before you start optimizing your workflow or application, you should establish a baseline. This means you should measure the performance of the current version of your workflow or application. This will give you a reference point to compare against. The benchmark should be chosen so that it is representative of the workload you are interested in. For example, it will likely not be useful to measure the performance of a workflow on a small data set if you intend to run it on a large data set. The memory usage and the runtime of the workflow will likely be very different, and hence the benchmark will not be representative. You should also make sure that you can run the benchmark easily and consistently. This means that you should automate the benchmarking process as much as possible. This will allow you to run the benchmark often and to compare the results. You should also make sure that you can reproduce the benchmark. This means that you should document the benchmark and the environment in which it was run. This will allow you to compare the results of the benchmark over time and to compare the results of the benchmark on different systems. You will find more information on this topic in the section on reproducibility .","title":"Benchmarking"},{"location":"optimization/#testing","text":"Having tests in place is vital before starting to optimize your workflow or application. You will want to make sure that everything still works after you make changes. You will find more information on this topic in the section on testing .","title":"Testing"},{"location":"optimization/#software-stack","text":"If you are using third party software, make sure you are using the version that is optimized for your system. This means that the software is compiled with the right compiler, with the right flags, and with the right libraries. Some software environments such as R will always build additional R packages from source. If you use a version of R built specifically for your system, the packages will also be built for your system. This can make a big difference in performance. For Python, you can use the Intel distribution of Python which is optimized for Intel processors. Intel also provides optimized versions of numpy, scipy, and many machine learning-related packages. When installing software from source, make sure to use the right compiler and the right flags. This can make a big difference in performance. See the section on compilers and their flags for more information. If you work on an HPC system, it is very likely that the software stack provided by the module system is already optimized for the system. Make sure to use the modules if you can. Even if several libraries offer the same functionality, they may not be equally efficient. For example, the Intel Math Kernel Library (MKL) is sometimes faster than OpenBLAS/LAPACK. Another example is the use of the Intel MPI library instead of Open MPI. The Intel MPI library is often faster than Open MPI, especially on Intel processors. However, you should experiment, since for some codes, Open MPI may be faster.","title":"Software stack"},{"location":"optimization/#your-own-code","text":"If the bulk of the time your workflow takes is spent in your own code, you you should consider optimizing your code. This can be done in many ways, use a faster compiler and compiler flags to optimize your code; a profiler to identify bottlenecks in your code; a faster algorithm and/or a more efficient data structure; a faster language; vectorization to speed up your code; parallelism to speed up your code.","title":"Your own code"},{"location":"optimization/#compilers-and-their-flags","text":"For Fortran, C and C++, make sure you are using the right compiler and the right flags. For example, the Intel compiler can often generate faster code than the GNU compiler. However, you should experiment, since for some codes, the GNU compiler may generate faster code. Make sure to compile with optimization flags such as -O3 and -xHost (Intel) or -march=native (GNU). Many other compiler flag influence performance, but that is outside the scope of this section. When using CMake, make sure to set the build type to Release to enable optimizations, i.e., cmake -DCMAKE_BUILD_TYPE=Release .. . CMake will then pass the appropriate flags to the compiler(s).","title":"Compilers and their flags"},{"location":"optimization/#profiling","text":"Your first priority is to identify the bottlenecks in your code. This can be done with a profiler. A profiler will tell you where your code spends most of its time. You can then focus on optimizing these parts of your code. You may think that you know where the bottlenecks are, but you may be surprised by the results of a profiler. It is always a good idea to profile your code before you start optimizing it. To make this concrete, suppose that your intuition tells you that you should improve the performance of a function, and you spend two days working on it, and are rewarded by a very impressive improvement by a factor of 10. This was excellent work. However, if your application spends only 5 % of its time in this function, what will be the runtime of the new version of your application if that is the only thing you change? If the total runtime of the original application is \\(t\\) , it means that its runtime after your optimization will be \\(0.95t + 0.5t/10 = 0.955t\\) , i.e., you improved the overall runtime by less than 5 %. In some circumstances this may be worth the effort you spent, but if you would have profiled your application, you might have chosen to spend your time differently. Remember that you should profile in representative circumstances, see the section on benchmarking for a more thorough discussion. There are many tools to profile your application, some open source, but commercial as well. Some tools can profile parallel applications. See the tools section for more information.","title":"Profiling"},{"location":"optimization/#algorithms-and-data-structures","text":"The choice of the algorithm you use to solve a particular problem, or the data structures you use to represent your data can have a profound impact on the performance of your application. To illustrate this, consider a very simple example, sorting a list of numbers. There are many algorithms to do this, and all result in the same sorted list. However, the choice of the algorithm will depend on the length of the list. A very simple algorithm is bubble sort which on average implies \\(O(N^2)\\) comparison operations where \\(N\\) is the length of the list. However, the quicksort algorithm that is a bit more sophisticated will on average only have to perform \\(O(N \\log N)\\) comparisons. For long lists, this makes a huge difference. For example, if the list has a 1,000 elements ( \\(N = 1000\\) ), then bubble sort will take a million comparisons, while quicksort will only require 3,000, and hence is a factor of 300 faster. The choice of data structures you make can also have a profound impact on performance. If the goal of the data structure is to hold, e.g., numbers to allow to check whether or not a number occurred previously, you can use a list or a set. Checking membership in a list will take \\(O(N)\\) comparisons on average, while checking membership of a set only takes \\(O(1)\\) (ideally). Just like the choice of algorithm in the previous example, this may have an important impact on performance. This section only draws your attention to the problem, it would be impossible to go into more detail, but it is a very good investment of your time to familiarize yourself with the subject. Many excellent books have been published on the subject, covering the most common algorithms, and a lot of articles have been published that address very specific algorithms and specialized data structures.","title":"Algorithms and data structures"},{"location":"optimization/#programming-languages","text":"Holy wars are waged on the subject of programming languages. Many of the discussions royally miss the point completely. In general compiled languages such as Fortran, C and C++ will produce faster applications. However, it is typically also more difficult to write applications in such a language then it is in R or Python. This implies that if your application is not performance critical, it would be a waste of time to write it in Fortran, C, or C++. Your time as a researcher is also quite valuable, so a short time to solution will be appreciated by the taxpayer. A good compromise is to implement the performance-critical parts of the application in a language such as Fortran, C, or C++, and wrap the resulting shared libraries so that they can be used from Python or R. This gives you the best of both worlds. Often, the work is already done for you. Consider the somewhat extreme example of machine learning. Frameworks such as TensorFlow or PyTorch allow you to write your code in Python at a high lever, while relying on libraries that were developed by HPC specialists under the hood. The execution time spent in pure Python code is completely negligible when compared to that spent on computations done by these core libraries. Again, profiling is crucial to determine what parts of your application make good candidates for reimplementation in a more performant programming language. Julia takes a somewhat different approach. The code you write will under the hood be translated to machine code before it is actually executed. Hence you get quite good performance out of the box when using Julia (well).","title":"Programming languages"},{"location":"optimization/#vectorization","text":"In scientific computing, core computations often consists of loops over arrays. If these loops implement matrix-vector or matrix-matrix operations, it is likely not a good idea to write such code. Consider using a library to represent these data structures that uses a BLAS (Basic Linear Algebra Subprograms) or LAPACK (Linear Algebra PACKage) library under the hood. These libraries have been developed with vectorization in mind, i.e., multiple iterations of the loops required for matrix-vector or matrix-matrix multiplication are done in parallel using wide registers. Depending on the numerical precision (single or double), the compiler flags used, as well as the hardware you compiled your code for that may range from 2 to 16. Since such operations are often at the core of your computations, ensuring that you benefit from vectorization may give you a significant performance benefit. In your own code, the compiler can often automatically apply vectorization on your own loops, given that it can prove that the iterations are independent of one another. To ensure that the compiler will attempt this, you would have to use the appropriate compiler flags. For the GNU compiler, this would be -ftree-vectorize , and -ftree-vectorizer-verbose=2 to see what the compiler did. For the Intel compiler, this would be -qopt-report=2 to see what the compiler did. Sometimes the compiler can not prove that vectorization is possible, i.e., that the iterations are independent. If you as a programmer knows for sure that although loop iterations are dependent, that dependence is on iterations that are not within the vector length, you can help the compiler by providing the appropriate OpenMP SIMD (Single Instruction, Multiple Data) directives. Again, this exceeds the scope of this section and you are referred to trainings on this specific topic.","title":"Vectorization"},{"location":"optimization/#parallelization","text":"Parallelization comes in many forms at various levels and complexity.","title":"Parallelization"},{"location":"optimization/#embarrassingly-parallel","text":"The most obvious and common situation is that you have to run your workflow or application on many different data sets, or with a variety of (hyper)parameter values. This form of parallelism is called \"embarrassingly parallel\" because it is very simple to implement. It is as simple as running your workflow or application on as many compute resources as you can get. The CPU (Central Processing Unit) of a modern laptop has multiple cores, each core capable of executing an application independent and in parallel with applications running on other cores. A simple shell script using GNU parallel can help you to run such workloads easily on your own system. On HPC clusters, more specialized tools exist that serve the same purpose, using the scheduler to run your workload in parallel. Some examples of such tools are atools and worker-ng . If your use case doesn't match this approach, or individual runs of your workflow or application require too much time then other options are available at the price of (often) much higher complexity and effort on your part.","title":"Embarrassingly parallel"},{"location":"optimization/#shared-memory-programming","text":"As mentioned before, modern CPUs have multiple cores, today ranging for typically 4 in your laptop to 128 in HPC compute nodes. Shared memory programming allows your application to utilize (potentially) all of these cores to perform computations in parallel, hence speeding up your application. Again, using the right libraries may give you a free lunch, similar to vectorization. Intel MKL and OpenBLAS/LAPACK will execute operations on multiple course if you link to the correct implementation. This is also the case for other libraries that use these under the hood. For example, the Armadillo C++ library will use BLAS and LAPACK libraries if available. The same applies to Python and R packages, e.g., numpy can use a BLAS library under the hood to perform matrix operations, hence using multiple cores for the computations. For Fortran, C, and C++, OpenMP is a good option to parallelize your code in a shared memory context. Typically, you annotate your source code with directives on, e.g., loop constructs to indicate to the compiler that these loops should be parallelized. The compiler will generate the appropriate instructions for you. This sounds suspiciously easy, and indeed, it is not so simple in practice to obtain good efficiency. Specifically for C++ a few more options are available. Many algorithms in the STL (Standard Template Library) can be executed in parallel with minimal changes to your source code. Of course, this is restrictions to those algorithms that support it. An alternative to OpenMP for C++ is TBB (Threading Building Blocks). This library is purely task oriented, with an excellent task scheduler under the hood. For Python, packages such as Cython and Numba will allow you to leverage multiple cores explicitly or implicitly, respectively. Again, you are referred to specific trainings on this subject.","title":"Shared memory programming"},{"location":"optimization/#gpgpus","text":"If your compute system is equiped with a graphics card that has compute capabilities (certain NVIDIA or AMD GPUs) then this is another level of parallelism that can be exploited. Again, certain libraries will do this for you under the hood, e.g., machine learning frameworks such as TensorFlow and PyTorch . More general frameworks such as cupy and Numba allow you to offload more general computations to the GPU. For Fortran, C, and C++ OpenACC or OpenMP may be an excellent solutions since compilers that implement these standards will generate appropriate code to transfer data to and from the GPU device, and run kernels. For C and C++ vendors of GPUs offer specific programming language extensions that work only on the specific hardware. This is CUDA for NVIDIA GPUs, and HIP for AMD hardware. This is a trade-off between (potentially) better performance and vendor lock-in. Specifically for C++ there are even more options: Data Parallel C++ (based on SYCL, developed by Intel) and the Kokkos library. Getting good performance on GPUs is not a trivial task due to data transport between host and device memory. Also, not all types of algorithms map well on the hardware architecture of such devices.","title":"GPGPUs"},{"location":"optimization/#using-multiple-computers","text":"Up to this point, parallelization was limited to the possibilities a single system offered, multiple cores and perhaps one or more GPUs. To further scale and use multiple compute nodes more work is required. Relatively simple option for some tasks exist for Python. Frameworks like Dask or PySpark allow you to distributes computations over multiple compute nodes. Note that these are not general solutions, but will support certain use cases only. If your problem fits one of those use cases, substantial gains can be made without too much effort. For Python, Fortran, C, and C++ Message Passing Interface (MPI) may be an option. This is a standard implemented in various libraries such as Intel MPI, Open MPI, MVAPICH and others. Using such a library requires a substantial rewrite of your code. The standard offers great flexibility and the potential for excellent parallel performance and scaling to a large number of nodes. To similarly parallelize code on multiple NVIDIA GPUs on multiple compute nodes, NCCL can be used. Conceptually, it is quite similar to MPI, but messages are exchanged between processes running on GPUs, rather than processes on the CPUs. Finally, Data Parallel C++ and Kokkos can also be used from C++ to parallelize your code over multiple nodes using MPI.","title":"Using multiple computers"},{"location":"references/","text":"References Style guides Style guides for various programming languages. C MISRA C2023 C++ C++ core guidelines Google C++ style guide MISRA C++2023 Fortran Fortran style guide Norman S. Clerman and Walter Spector, \"Modern Fortran: style and usage\" , Cambridge University Press, 2011 Python PEP8 Python style guide R tidyverse style guide Programming language standards C standards C++ standards Fortran standards Programming languages such as Python and R don't have formal standards. General best practices General resources that discuss best practices in software engineering. Brain Kernighan and Rob Pike, \"The practice of programming\" , Addison-Wesley, 1999 Martin Fowler, \"Refactoring: Improving the Design of Existing Code\" , Addison Wesley, 1999 Erich Gamma, Richard Helm, Ralpha Johnson and John Vlissides, \"Design patterns: elements of reusable object-oriented software\" , Addison-Wesley, 1994 Robert C. Martin, \"Clean code: a handbook of agile software craftsmanship\" , Pearson, 2008 Best practices for scientific computing Greg Wilson, Jennifer Bryan, Karen Cranston et al., Good enough practices in scientific computing , PLoS Computational Biology, 13(6):e1005510, 2017 G. Wilson, D.A. Aruliah, C.T. Browni et al., Best practices for scientific computing , PLoS Biology, 12(1):e1001745, 2014 Software licenses Discussion of how to choose an appropriate license on the GNU website. Scientific software Some references that are more specific to scientific software development. Task Force Sub Group 3 - Review of Software Quality Attributes and Characteristics Development ecosystem The state of the development ecosystem in 2023 Algorithms and data structures Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein, \"Introduction to algorithms\" , MIT Press, 2009 (3rd edition) Robert Sedgewick and Kevin Wayne, \"Algorithms\" , Addison-Wesley, 2011 (4th edition) Awesome algorithms","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#style-guides","text":"Style guides for various programming languages.","title":"Style guides"},{"location":"references/#c","text":"MISRA C2023","title":"C"},{"location":"references/#c_1","text":"C++ core guidelines Google C++ style guide MISRA C++2023","title":"C++"},{"location":"references/#fortran","text":"Fortran style guide Norman S. Clerman and Walter Spector, \"Modern Fortran: style and usage\" , Cambridge University Press, 2011","title":"Fortran"},{"location":"references/#python","text":"PEP8 Python style guide","title":"Python"},{"location":"references/#r","text":"tidyverse style guide","title":"R"},{"location":"references/#programming-language-standards","text":"C standards C++ standards Fortran standards Programming languages such as Python and R don't have formal standards.","title":"Programming language standards"},{"location":"references/#general-best-practices","text":"General resources that discuss best practices in software engineering. Brain Kernighan and Rob Pike, \"The practice of programming\" , Addison-Wesley, 1999 Martin Fowler, \"Refactoring: Improving the Design of Existing Code\" , Addison Wesley, 1999 Erich Gamma, Richard Helm, Ralpha Johnson and John Vlissides, \"Design patterns: elements of reusable object-oriented software\" , Addison-Wesley, 1994 Robert C. Martin, \"Clean code: a handbook of agile software craftsmanship\" , Pearson, 2008","title":"General best practices"},{"location":"references/#best-practices-for-scientific-computing","text":"Greg Wilson, Jennifer Bryan, Karen Cranston et al., Good enough practices in scientific computing , PLoS Computational Biology, 13(6):e1005510, 2017 G. Wilson, D.A. Aruliah, C.T. Browni et al., Best practices for scientific computing , PLoS Biology, 12(1):e1001745, 2014","title":"Best practices for scientific computing"},{"location":"references/#software-licenses","text":"Discussion of how to choose an appropriate license on the GNU website.","title":"Software licenses"},{"location":"references/#scientific-software","text":"Some references that are more specific to scientific software development. Task Force Sub Group 3 - Review of Software Quality Attributes and Characteristics","title":"Scientific software"},{"location":"references/#development-ecosystem","text":"The state of the development ecosystem in 2023","title":"Development ecosystem"},{"location":"references/#algorithms-and-data-structures","text":"Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein, \"Introduction to algorithms\" , MIT Press, 2009 (3rd edition) Robert Sedgewick and Kevin Wayne, \"Algorithms\" , Addison-Wesley, 2011 (4th edition) Awesome algorithms","title":"Algorithms and data structures"},{"location":"reproducibility/","text":"Reproducibility A very important aspect of scientific research is the ability to reproduce the results of an experiment. There are a number of factors that can affect the reproducibility of an experiment, including the data used; the versions of the software used, including your own code and any third-party libraries; the workflow used to run the experiment, including the order in which scripts are run; the (hyper)parameters used for the experiment. Not only should you be able to reproduce your own results, but others should be able to do so as well, e.g., the reviewers of a paper that you submit for publication. This is important for the credibility of your work. This is also crucial to allow other researchers to build on your work. If they cannot reproduce your results, it is hard to assess the potential improvements of their own work. Data The first step in ensuring that your results are reproducible is to keep track of the data used in your experiments. Ideally, your data resides in a data repository that gives long-term access to the data. Some of these repositories are domain-specific, but there are also general repositories that can be used to store data from any domain. Some of these repositories are built to support the FAIR principles, which state that data should be Findable; Accessible; Interoperable; Reusable. Findable means that the data should be easy to find for both humans and computers, e.g., by using a unique identifier such as a digital object identifier (DOI), but also by adding metadata to the data. Accessible means that the data should be easy to access, e.g., by providing a download link, but also by providing the data in a format that is easy to use. Interoperable means that the data should be easy to combine with other data, e.g., by using a standard format, but also by providing metadata that describes the data. Reusable means that the data should be easy to reuse, e.g., by providing metadata that describes the data, but also by providing documentation that explains how the data was generated. Of course you might not want to share all or any of your data while a research project is ongoing, so many repositories allow you to keep your data private until you are ready to share it. However, it is a good idea to start thinking about where you will store your data once the project is finished, as this will make it easier to share your results with others. Many funding agencies now require that data be made available after a project is finished, and even require a data management plan as part of a project proposal. Typically, storing data sets directly in your version control system is not possible due to restrictions on the size of files that can be stored. However, you can use a tool such as DVC to manage versions of your data. DVC stores the data in a separate location, and keeps track of the versions of the data in a text file that can be version controlled along with the other artifacts of your experiments. Software versions To ensure that you can reproduce results generated using your own code, it is important to use a version control system. Using tags appropriately can help you to keep track of the versions used to generate results for, e.g., the submission of a paper. For more information on version control, see the section on version control . In addition to version control for your own code, it is also important to keep track of the versions of third-party libraries that you use. This can typically be done by using a package manager, such as miniconda or Poetry for Python. Package managers typically allow to specify requirements in a text file that can be versioned along with your project. For C, C++ or Fortran, the build system, e.g., CMake , can be used to specify the versions of libraries used. At the level of entire software packages, but also for libraries, the Spack package manager can be used to specify the exact versions of software used. Spack can be used to install software in a controlled environment, ensuring that the software is built with the correct dependencies and compiler options. Going one step further you can use a containerization tool, such as Docker , Podman , or Apptainer . These tools allow you to specify the exact environment in which your code is run, including the operating system and the versions of all software used. The container definition is a simple text file that can be version controlled along with your code. This makes it easy to share your code and the environment in which it was run with others, ensuring that they can reproduce your results. Clearly, the environment specification or the container definition should mention explicit version numbers for all software used, including the operating system. This ensures that the environment in which your code is run is exactly the same which ensures that your results are reproducible. Obviously, the environment specification or the container definition should be version controlled along with your code. This ensures that you can reproduce the results of your experiments at any time in the future. Important note on performance: using package managers or containerization tools such as Docker can have an impact on the performance of your code. This is because the software is built with specific compiler options and dependencies that may not be optimal for your specific use case. For this reason, it is important to test the performance of your code in the environment in which it will be run, and to optimize the build options if necessary. For containers, it is best practice to build those on a similar environment as the one on which they will be run. This is especially important if you are running your code on a high-performance computing (HPC) system, where the hardware and software environment can be quite different from your own workstation. However, if you use Linux package managers for performance-critical applications while building your container, the results are likely to be suboptimal. Workflows An experiment or series of experiments typically involves a number of steps: data ingestion; data preprocessing; running one or more applications; postprocessing the results. The bare minimum you should do is to have an accurate, complete description of the steps to be taken. Better still, write a script that automates the entire process. This can be done in various ways, ranging from quite straightforward shell scripts to more complex workflows using tools such as Nextflow , or Snakemake . An additional advantage of using a workflow management system is that it can help you to redo only the parts of the workflow that are affected by changes in the input data or the code. This can save a lot of time when you are developing your code, as you do not have to rerun the entire workflow every time you make a change. Of course, the development overhead of creating a full fledged workflow will be only worth the effort if the workflow is involved enough. For simple experiments, a shell script may be sufficient. Note that the workflow can now be version controlled along with your code, and the environment specification or the container definition. This ensures that you can reproduce the results of your experiments at any time in the future. Parameters The final aspect of reproducibility is the parameters used for the experiment. These can be the parameters of data preprocessing, the parameters of the application, or the parameters of the postprocessing. It is important to keep track of these parameters, as they can have a significant impact on the results of the experiment. It is therefore good practice to store the parameters used for an experiment in one or more separate files, and to version control these files. If you develop your own software, it is good practice to ensure that the application can be run with parameters stored in a configuration file. Libraries exist for many programming languages that can be used to read configuration files, e.g., Hydra for Python, or for Python, or libconfig for C/C++. Note that some preprocessing steps and applications may use a random number generator. In this case, it is important to store the seed of the random number generator along with the parameters used for the experiment. This ensures that the results of the experiment can be reproduced exactly.","title":"Reproducibility"},{"location":"reproducibility/#reproducibility","text":"A very important aspect of scientific research is the ability to reproduce the results of an experiment. There are a number of factors that can affect the reproducibility of an experiment, including the data used; the versions of the software used, including your own code and any third-party libraries; the workflow used to run the experiment, including the order in which scripts are run; the (hyper)parameters used for the experiment. Not only should you be able to reproduce your own results, but others should be able to do so as well, e.g., the reviewers of a paper that you submit for publication. This is important for the credibility of your work. This is also crucial to allow other researchers to build on your work. If they cannot reproduce your results, it is hard to assess the potential improvements of their own work.","title":"Reproducibility"},{"location":"reproducibility/#data","text":"The first step in ensuring that your results are reproducible is to keep track of the data used in your experiments. Ideally, your data resides in a data repository that gives long-term access to the data. Some of these repositories are domain-specific, but there are also general repositories that can be used to store data from any domain. Some of these repositories are built to support the FAIR principles, which state that data should be Findable; Accessible; Interoperable; Reusable. Findable means that the data should be easy to find for both humans and computers, e.g., by using a unique identifier such as a digital object identifier (DOI), but also by adding metadata to the data. Accessible means that the data should be easy to access, e.g., by providing a download link, but also by providing the data in a format that is easy to use. Interoperable means that the data should be easy to combine with other data, e.g., by using a standard format, but also by providing metadata that describes the data. Reusable means that the data should be easy to reuse, e.g., by providing metadata that describes the data, but also by providing documentation that explains how the data was generated. Of course you might not want to share all or any of your data while a research project is ongoing, so many repositories allow you to keep your data private until you are ready to share it. However, it is a good idea to start thinking about where you will store your data once the project is finished, as this will make it easier to share your results with others. Many funding agencies now require that data be made available after a project is finished, and even require a data management plan as part of a project proposal. Typically, storing data sets directly in your version control system is not possible due to restrictions on the size of files that can be stored. However, you can use a tool such as DVC to manage versions of your data. DVC stores the data in a separate location, and keeps track of the versions of the data in a text file that can be version controlled along with the other artifacts of your experiments.","title":"Data"},{"location":"reproducibility/#software-versions","text":"To ensure that you can reproduce results generated using your own code, it is important to use a version control system. Using tags appropriately can help you to keep track of the versions used to generate results for, e.g., the submission of a paper. For more information on version control, see the section on version control . In addition to version control for your own code, it is also important to keep track of the versions of third-party libraries that you use. This can typically be done by using a package manager, such as miniconda or Poetry for Python. Package managers typically allow to specify requirements in a text file that can be versioned along with your project. For C, C++ or Fortran, the build system, e.g., CMake , can be used to specify the versions of libraries used. At the level of entire software packages, but also for libraries, the Spack package manager can be used to specify the exact versions of software used. Spack can be used to install software in a controlled environment, ensuring that the software is built with the correct dependencies and compiler options. Going one step further you can use a containerization tool, such as Docker , Podman , or Apptainer . These tools allow you to specify the exact environment in which your code is run, including the operating system and the versions of all software used. The container definition is a simple text file that can be version controlled along with your code. This makes it easy to share your code and the environment in which it was run with others, ensuring that they can reproduce your results. Clearly, the environment specification or the container definition should mention explicit version numbers for all software used, including the operating system. This ensures that the environment in which your code is run is exactly the same which ensures that your results are reproducible. Obviously, the environment specification or the container definition should be version controlled along with your code. This ensures that you can reproduce the results of your experiments at any time in the future. Important note on performance: using package managers or containerization tools such as Docker can have an impact on the performance of your code. This is because the software is built with specific compiler options and dependencies that may not be optimal for your specific use case. For this reason, it is important to test the performance of your code in the environment in which it will be run, and to optimize the build options if necessary. For containers, it is best practice to build those on a similar environment as the one on which they will be run. This is especially important if you are running your code on a high-performance computing (HPC) system, where the hardware and software environment can be quite different from your own workstation. However, if you use Linux package managers for performance-critical applications while building your container, the results are likely to be suboptimal.","title":"Software versions"},{"location":"reproducibility/#workflows","text":"An experiment or series of experiments typically involves a number of steps: data ingestion; data preprocessing; running one or more applications; postprocessing the results. The bare minimum you should do is to have an accurate, complete description of the steps to be taken. Better still, write a script that automates the entire process. This can be done in various ways, ranging from quite straightforward shell scripts to more complex workflows using tools such as Nextflow , or Snakemake . An additional advantage of using a workflow management system is that it can help you to redo only the parts of the workflow that are affected by changes in the input data or the code. This can save a lot of time when you are developing your code, as you do not have to rerun the entire workflow every time you make a change. Of course, the development overhead of creating a full fledged workflow will be only worth the effort if the workflow is involved enough. For simple experiments, a shell script may be sufficient. Note that the workflow can now be version controlled along with your code, and the environment specification or the container definition. This ensures that you can reproduce the results of your experiments at any time in the future.","title":"Workflows"},{"location":"reproducibility/#parameters","text":"The final aspect of reproducibility is the parameters used for the experiment. These can be the parameters of data preprocessing, the parameters of the application, or the parameters of the postprocessing. It is important to keep track of these parameters, as they can have a significant impact on the results of the experiment. It is therefore good practice to store the parameters used for an experiment in one or more separate files, and to version control these files. If you develop your own software, it is good practice to ensure that the application can be run with parameters stored in a configuration file. Libraries exist for many programming languages that can be used to read configuration files, e.g., Hydra for Python, or for Python, or libconfig for C/C++. Note that some preprocessing steps and applications may use a random number generator. In this case, it is important to store the seed of the random number generator along with the parameters used for the experiment. This ensures that the results of the experiment can be reproduced exactly.","title":"Parameters"},{"location":"syntax_vs_semantics/","text":"Syntax versus semantics \"Syntax\" and \"semantics\" are two terms we will use often in this training. Most of you are probably familiar with these concepts, but just to make sure, let's define them briefly. The syntax of a programming language is its grammar, i.e., the rules the source code text must satisfy in order to be considered syntactically correct. Let's illustrate that with an example from natural language. The sentence \"the dog barks\" is syntactically correct, while \"the dog bark\" is not. Semantics on the other hand has to do with meaning, or interpretation. Again drawing on natural language examples, the sentence \"the dog spoke\" is syntactically correct, but the semantics is wrong. Obviously, dogs do not speak, except in fairy tales. The sentence \"the dog barks\" is both syntactically and semantically correct. The following code fragments would be syntactically incorrect. program syntax_error implicit none use, intrinsic :: iso_fortran_env, only : output_unit write (unit=output_unit, fmt='(A)') 'hello world' end program syntax_error This Fortran program has a syntax error that is dutifully reported by the gfortran compiler as: ~~~bash $ gfortran -c syntax_error.f90 syntax_error.f90:3.57: use, intrinsic :: iso_fortran_env, only : output_unit 1 syntax_error.f90:2.17: implicit none 2 Error: USE statement at (1) cannot follow IMPLICIT NONE statement at (2) The following Fortran function definition has a semantic error, although it is syntactically correct. ~~~~fortran integer function fac(n) implicit none integer, intent(IN) :: n integer :: i fac = 1 do i = 0, n fac = fac*i end do end function fac The code fragment above will be compiled without errors or even warnings, but the function will return zero when invoked, regardless of the argument passed to it. This illustrates an important point: syntax errors are always caught by the compiler. Although having to fix syntax errors is a nuisance, it is relatively easy. Most semantic errors are not caught by the compiler, so these are harder to spot and more difficult to fix. Compilers do offer some help detecting certain classes of semantic error, as you can see in the Tools section.","title":"Syntax vs. semantics"},{"location":"syntax_vs_semantics/#syntax-versus-semantics","text":"\"Syntax\" and \"semantics\" are two terms we will use often in this training. Most of you are probably familiar with these concepts, but just to make sure, let's define them briefly. The syntax of a programming language is its grammar, i.e., the rules the source code text must satisfy in order to be considered syntactically correct. Let's illustrate that with an example from natural language. The sentence \"the dog barks\" is syntactically correct, while \"the dog bark\" is not. Semantics on the other hand has to do with meaning, or interpretation. Again drawing on natural language examples, the sentence \"the dog spoke\" is syntactically correct, but the semantics is wrong. Obviously, dogs do not speak, except in fairy tales. The sentence \"the dog barks\" is both syntactically and semantically correct. The following code fragments would be syntactically incorrect. program syntax_error implicit none use, intrinsic :: iso_fortran_env, only : output_unit write (unit=output_unit, fmt='(A)') 'hello world' end program syntax_error This Fortran program has a syntax error that is dutifully reported by the gfortran compiler as: ~~~bash $ gfortran -c syntax_error.f90 syntax_error.f90:3.57: use, intrinsic :: iso_fortran_env, only : output_unit 1 syntax_error.f90:2.17: implicit none 2 Error: USE statement at (1) cannot follow IMPLICIT NONE statement at (2) The following Fortran function definition has a semantic error, although it is syntactically correct. ~~~~fortran integer function fac(n) implicit none integer, intent(IN) :: n integer :: i fac = 1 do i = 0, n fac = fac*i end do end function fac The code fragment above will be compiled without errors or even warnings, but the function will return zero when invoked, regardless of the argument passed to it. This illustrates an important point: syntax errors are always caught by the compiler. Although having to fix syntax errors is a nuisance, it is relatively easy. Most semantic errors are not caught by the compiler, so these are harder to spot and more difficult to fix. Compilers do offer some help detecting certain classes of semantic error, as you can see in the Tools section.","title":"Syntax versus semantics"},{"location":"training/","text":"Training There are various training sessions that are of interest if you are into scientific computing and want to learn more about best practices and tools. Training schedules Several organizations offer training programs, here are some of them: Vlaams Supercomputer Centrum (VSC) EuroCC Gauss Centre for Supercomputing (GCS) POP CoE training Per topics Note that this is a list of topics that are mentioned in this best-practices guide. The links are to the training site, but you many not find schedules there. Performance optimization Code optimization POP CoE training","title":"Training"},{"location":"training/#training","text":"There are various training sessions that are of interest if you are into scientific computing and want to learn more about best practices and tools.","title":"Training"},{"location":"training/#training-schedules","text":"Several organizations offer training programs, here are some of them: Vlaams Supercomputer Centrum (VSC) EuroCC Gauss Centre for Supercomputing (GCS) POP CoE training","title":"Training schedules"},{"location":"training/#per-topics","text":"Note that this is a list of topics that are mentioned in this best-practices guide. The links are to the training site, but you many not find schedules there.","title":"Per topics"},{"location":"training/#performance-optimization","text":"Code optimization POP CoE training","title":"Performance optimization"},{"location":"version_control/","text":"Version control Version control is a very important aspect of software development. In short, it allows to answer the following questions. What was changed? When was it changed? Who made the change? Why was the change made? Using a version control system you can compare versions of your code, and if necessary, revert to a previous version. It is good practice to host you repositories on a service such as GitHub , GitLab or a hosting service provided by your organization. These environments all facilitate collaboration on software projects and make it easy to work in teams. You can find a PowerPoint presentation that introduces git and hosting services in the training materials repository for Version control with git .","title":"Version control"},{"location":"version_control/#version-control","text":"Version control is a very important aspect of software development. In short, it allows to answer the following questions. What was changed? When was it changed? Who made the change? Why was the change made? Using a version control system you can compare versions of your code, and if necessary, revert to a previous version. It is good practice to host you repositories on a service such as GitHub , GitLab or a hosting service provided by your organization. These environments all facilitate collaboration on software projects and make it easy to work in teams. You can find a PowerPoint presentation that introduces git and hosting services in the training materials repository for Version control with git .","title":"Version control"},{"location":"testing/","text":"Testing Testing your software is an essential part of the development process. It is important to have tests in place before you start optimizing or rewriting your code. The following sections deal with specific aspects of testing and types of tests: testing as experiments ; unit testing ; functional testing ; code coverage .","title":"Overview"},{"location":"testing/#testing","text":"Testing your software is an essential part of the development process. It is important to have tests in place before you start optimizing or rewriting your code. The following sections deal with specific aspects of testing and types of tests: testing as experiments ; unit testing ; functional testing ; code coverage .","title":"Testing"},{"location":"testing/code_coverage/","text":"Code coverage Introduction Unit tests are very useful to formulate fine-grained test to check the functionality of functions and methods. Unit tests check for edge and corner cases, but also for handling of error conditions such as exceptions being thrown. This is of course very useful in itself, but using a run of the complete unit test suite can also provide a good test to see whether all functions and methods are called, and all codes paths in the code get executed. Code coverage tools will instrument your code, run it, and provide feedback on regions of code that are not executed doing that run. Since we claim that code that is not tested is not correct, coverage provided by running all the unit tests should in fact be (close to) 100 %. If code coverage is insufficient, more unit tests should be added to the test suite. Best practices Maintaining code over long periods of time is quite expensive. The larger the code base, the more effort has to be spent on keeping code up to date. If some parts of the code are never used, that adds to this burden without return on investment. Updates are an issue as well, since some unused parts of the code may get out of sync with respect to the parts that are executed regularly. If someone starts using the abandoned part of the code, interesting bugs may creep into her code. Hence code that is not used is best removed from the code base. If you use a version control system and informative commit messages, it is quite easy to recover that code later when it is unexpectedly required. An important concern when writing tests for your software project is whether or not all branches in function, and indeed all functions are tested. Figuring out by hand whether that is the case is pretty hard for sizable projects. Fortunately, software tools are available for checking which parts of the code base are executed and which are not. For many programming languages, one has to resort to third party tools, but the compilers for C, C++ and Fortran support this out of the box. The first step in the workflow is to instrument the code with instructions to do the bookkeeping for reporting which lines of code have been executed. Compilers have options to do that automatically, so this can easily be incorporated into the build process by adding a make target specific for a code coverage build. The second step is to execute the software, so that a report is generated. In case you wonder how to run your code to get the most useful report, this depends on your goal. If you want to detect code that is likely not executed in applications, running a number of typical use cases are the best way to go. On the other hand, if you want to verify that you have a comprehensive set of unit tests, execute those. The third step is to inspect that report. Typically, you will get summary information, e.g., the percentage of the code covered in each file. In addition, each individual file can be inspected on a line by line basis. Lines that have not been executed are clearly marked, so that they are easy to spot. Finally, you decide to either weed out the lines if they are dead code, i.e., code that will never be executed in the context of your project, or to create additional unit tests if that code will be executed but is not tested yet. Since code coverage tests produce artifacts, it is best to add rules to remove these artifacts to your make file. This ensures you start with a clean slate when rebuilding. Code coverage assessment is an important tool for delivering good quality code, and goes hand in hand with unit testing and functional testing.","title":"Code coverage"},{"location":"testing/code_coverage/#code-coverage","text":"","title":"Code coverage"},{"location":"testing/code_coverage/#introduction","text":"Unit tests are very useful to formulate fine-grained test to check the functionality of functions and methods. Unit tests check for edge and corner cases, but also for handling of error conditions such as exceptions being thrown. This is of course very useful in itself, but using a run of the complete unit test suite can also provide a good test to see whether all functions and methods are called, and all codes paths in the code get executed. Code coverage tools will instrument your code, run it, and provide feedback on regions of code that are not executed doing that run. Since we claim that code that is not tested is not correct, coverage provided by running all the unit tests should in fact be (close to) 100 %. If code coverage is insufficient, more unit tests should be added to the test suite.","title":"Introduction"},{"location":"testing/code_coverage/#best-practices","text":"Maintaining code over long periods of time is quite expensive. The larger the code base, the more effort has to be spent on keeping code up to date. If some parts of the code are never used, that adds to this burden without return on investment. Updates are an issue as well, since some unused parts of the code may get out of sync with respect to the parts that are executed regularly. If someone starts using the abandoned part of the code, interesting bugs may creep into her code. Hence code that is not used is best removed from the code base. If you use a version control system and informative commit messages, it is quite easy to recover that code later when it is unexpectedly required. An important concern when writing tests for your software project is whether or not all branches in function, and indeed all functions are tested. Figuring out by hand whether that is the case is pretty hard for sizable projects. Fortunately, software tools are available for checking which parts of the code base are executed and which are not. For many programming languages, one has to resort to third party tools, but the compilers for C, C++ and Fortran support this out of the box. The first step in the workflow is to instrument the code with instructions to do the bookkeeping for reporting which lines of code have been executed. Compilers have options to do that automatically, so this can easily be incorporated into the build process by adding a make target specific for a code coverage build. The second step is to execute the software, so that a report is generated. In case you wonder how to run your code to get the most useful report, this depends on your goal. If you want to detect code that is likely not executed in applications, running a number of typical use cases are the best way to go. On the other hand, if you want to verify that you have a comprehensive set of unit tests, execute those. The third step is to inspect that report. Typically, you will get summary information, e.g., the percentage of the code covered in each file. In addition, each individual file can be inspected on a line by line basis. Lines that have not been executed are clearly marked, so that they are easy to spot. Finally, you decide to either weed out the lines if they are dead code, i.e., code that will never be executed in the context of your project, or to create additional unit tests if that code will be executed but is not tested yet. Since code coverage tests produce artifacts, it is best to add rules to remove these artifacts to your make file. This ensures you start with a clean slate when rebuilding. Code coverage assessment is an important tool for delivering good quality code, and goes hand in hand with unit testing and functional testing.","title":"Best practices"},{"location":"testing/functional_testing/","text":"Functional testing Introduction Unit testing is a great help during the development process. It will help us spot problems introduced by code changes immediately after they have been introduced. They help test the functionality at the level of individual functions and methods. However, to test an entire application that is, e.g., run from the command line with various parameters, unit tests are not really the right tool. The literature on software development refers to this type of testing as functional testing. Tests are more coarse grained, and are likely to take longer to run than you are comfortable with during an intensive edit/test/commit development cycle. Hence it is acceptable to run functional tests less often, for instance only when a feature has been added to the software, or a bug has been fixed that may have involved a considerable number of file edits and commits. We rely on unit testing to ensure that this process didn't break low-level integrity of the code. Ideally, functional testing is done automatically for a release with an online tool such as Travis CI or GitHub Actions for continuous integration. However, you can also do functional testing locally using, e.g., shunit2 or CTest . Functional testing can detect code defects that would go unnoticed by unit testing, so both testing strategies are complementing one another. Best practices Unit testing is an invaluable help for the developer since it catches bugs introduced when the code base changes. Tests can be executed easily and are run frequently. However, unit tests typically concentrate on the low level functionality of the software project. They test whether individual functions behave as expected. This is white box testing, since the tests are developed with access to the \"innards\" of the software under test. In some circumstances, this may be all that is required, e.g., when developing a relatively small or very focused library. In many cases though, unit testing is best supplemented by functional testing. The point of view of functional testing is opposite to that of unit testing since functional tests will focus on the application as a whole. Are the results for a sophisticated use case reproduced as expected? Does the application's user interface, command line interface (CLI), or graphical user interface (GUI) behave as expected? Are options handled as expected? This is often called black box testing since only the user interface is accessed. Functional testing can also be applied to third party applications that are part of a workflow. For instance, suppose that your application relies on the output of another application not developed by you. If the output format of that application changes from one version to the next, running a functional test will make clear whether there is an impact on your workflow, and you may fix problems by adapting your application. The best way to do functional testing is by using a continuous integration workflow. When the functional tests are run, first a container is prepared with the required operating system and software stack. Next, your software is built within the container, so that the environment is completely controlled. If the build succeeds, tests are run. A report is generated to show failures if they occur. Note that it is possible to set up a matrix of operating system versions and compiler versions to ensure that your code will build and executed cleanly on a wide range of software platforms. Both GitHub and Gitlab support continuous integration that can be used for functional testing. The question remains how to code the actual tests that will be executed by the continuous integration system. A convenient way is to reuse the unit test paradigm, but now on the level of the shell. In other words, the unit tests will be relatively short shell scripts that invoke your application using various parameters and input data, and verify the results. The shunit2 framework provides a nice framework for this purpose. It provides similar functionality as the unit testing frameworks for specific programming languages. However, from the point of view of the software project this is black box, rather than white box testing. The same concerns as for unit testing apply. For instance, it is important that the tests cover the use cases as well as possible. Here too, code coverage can be a great help to detect which application aspects are tested, and for which additional tests need to be implemented to improve the coverage. Examples CMake tests shunit2 tests CI for testing","title":"Functional testing"},{"location":"testing/functional_testing/#functional-testing","text":"","title":"Functional testing"},{"location":"testing/functional_testing/#introduction","text":"Unit testing is a great help during the development process. It will help us spot problems introduced by code changes immediately after they have been introduced. They help test the functionality at the level of individual functions and methods. However, to test an entire application that is, e.g., run from the command line with various parameters, unit tests are not really the right tool. The literature on software development refers to this type of testing as functional testing. Tests are more coarse grained, and are likely to take longer to run than you are comfortable with during an intensive edit/test/commit development cycle. Hence it is acceptable to run functional tests less often, for instance only when a feature has been added to the software, or a bug has been fixed that may have involved a considerable number of file edits and commits. We rely on unit testing to ensure that this process didn't break low-level integrity of the code. Ideally, functional testing is done automatically for a release with an online tool such as Travis CI or GitHub Actions for continuous integration. However, you can also do functional testing locally using, e.g., shunit2 or CTest . Functional testing can detect code defects that would go unnoticed by unit testing, so both testing strategies are complementing one another.","title":"Introduction"},{"location":"testing/functional_testing/#best-practices","text":"Unit testing is an invaluable help for the developer since it catches bugs introduced when the code base changes. Tests can be executed easily and are run frequently. However, unit tests typically concentrate on the low level functionality of the software project. They test whether individual functions behave as expected. This is white box testing, since the tests are developed with access to the \"innards\" of the software under test. In some circumstances, this may be all that is required, e.g., when developing a relatively small or very focused library. In many cases though, unit testing is best supplemented by functional testing. The point of view of functional testing is opposite to that of unit testing since functional tests will focus on the application as a whole. Are the results for a sophisticated use case reproduced as expected? Does the application's user interface, command line interface (CLI), or graphical user interface (GUI) behave as expected? Are options handled as expected? This is often called black box testing since only the user interface is accessed. Functional testing can also be applied to third party applications that are part of a workflow. For instance, suppose that your application relies on the output of another application not developed by you. If the output format of that application changes from one version to the next, running a functional test will make clear whether there is an impact on your workflow, and you may fix problems by adapting your application. The best way to do functional testing is by using a continuous integration workflow. When the functional tests are run, first a container is prepared with the required operating system and software stack. Next, your software is built within the container, so that the environment is completely controlled. If the build succeeds, tests are run. A report is generated to show failures if they occur. Note that it is possible to set up a matrix of operating system versions and compiler versions to ensure that your code will build and executed cleanly on a wide range of software platforms. Both GitHub and Gitlab support continuous integration that can be used for functional testing. The question remains how to code the actual tests that will be executed by the continuous integration system. A convenient way is to reuse the unit test paradigm, but now on the level of the shell. In other words, the unit tests will be relatively short shell scripts that invoke your application using various parameters and input data, and verify the results. The shunit2 framework provides a nice framework for this purpose. It provides similar functionality as the unit testing frameworks for specific programming languages. However, from the point of view of the software project this is black box, rather than white box testing. The same concerns as for unit testing apply. For instance, it is important that the tests cover the use cases as well as possible. Here too, code coverage can be a great help to detect which application aspects are tested, and for which additional tests need to be implemented to improve the coverage.","title":"Best practices"},{"location":"testing/functional_testing/#examples","text":"CMake tests shunit2 tests CI for testing","title":"Examples"},{"location":"testing/testing_as_experiments/","text":"Testing as scientific experiments If you are a scientist, or at least interested in the subject, you know about the scientific method. Simplified, the process works as follows: 1. You make a number of observations. 1. You formulate a hypothesis that has predictive power. 1. You design experiments to test predictions made by the hypothesis. 1. If an experiment succeeds, your confidence in the hypothesis increases. 1. However, if an experiment fails, you know for certain that there is a problem and you reformulate your hypothesis, possibly gathering some more observations to do so. It is worth pointing out that experiments are actually designed to disprove the hypothesis rather than to confirm it. A quote of the philosopher of science Karl Popper illustrates this. If we are uncritical we shall always find what we want: we shall look for, and find, confirmations, and we shall look away from, and not see, whatever might be dangerous to our pet theories. If you substitute \"pet theories\" in the above by \"pet implementations\", it is quite clear that the same maxims apply to software testing as to scientific research. Another quote from his famous book \"The logic of scientific discovery\" (1934) also translates well to software testing. ...no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white. Paraphrased, you can read that as \"... no matter how many tests your software may pass, it doesn't justify the conclusion that it is correct\".","title":"Testing as experiments"},{"location":"testing/testing_as_experiments/#testing-as-scientific-experiments","text":"If you are a scientist, or at least interested in the subject, you know about the scientific method. Simplified, the process works as follows: 1. You make a number of observations. 1. You formulate a hypothesis that has predictive power. 1. You design experiments to test predictions made by the hypothesis. 1. If an experiment succeeds, your confidence in the hypothesis increases. 1. However, if an experiment fails, you know for certain that there is a problem and you reformulate your hypothesis, possibly gathering some more observations to do so. It is worth pointing out that experiments are actually designed to disprove the hypothesis rather than to confirm it. A quote of the philosopher of science Karl Popper illustrates this. If we are uncritical we shall always find what we want: we shall look for, and find, confirmations, and we shall look away from, and not see, whatever might be dangerous to our pet theories. If you substitute \"pet theories\" in the above by \"pet implementations\", it is quite clear that the same maxims apply to software testing as to scientific research. Another quote from his famous book \"The logic of scientific discovery\" (1934) also translates well to software testing. ...no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white. Paraphrased, you can read that as \"... no matter how many tests your software may pass, it doesn't justify the conclusion that it is correct\".","title":"Testing as scientific experiments"},{"location":"testing/unit_testing/","text":"Unit testing Introduction Testing is obviously a way to ensure that at least part of the functionality behaves as expected. However, good tests can provide more than that, they can help ensure that changes to the code base don't introduce defects. A code base evolves dynamically, potentially over a long period of time. Adding new features to software is typically quite error prone, and might inadvertently break some use cases. In order to minimize this risk should have a sizable collection of tests available that check whether results are as expected, and be able to run those easily and frequently as part of your development cycle. Unit tests are an excellent approach. They consist of many small fragments of code that each test very specific aspects of the functionality of a library. Using frameworks such as pFUnit for Fortran, CUnit for C, Catch2 for C++, pytest for Python, or testthat for R will take care of the \"bookkeeping\" and ensure that running tests is effortless. Best practices Very often code defects are introduced while the code evolves to implement new features or even to fix emerging problems. Without proper testing these new bugs may go unnoticed for a while, even until after the release of a new version of a software project. Hence it is good practice to have a set of tests at hand for regression testing, i.e., testing whether what used to work correctly, still does. It is a very important aspect of a software project with a non-trivial life-cycle. Testing code by running an application and visually inspecting the results is very error-prone. It is all too easy to miss a problem when the output is fairly long, as would be the case for most software projects. Hence a different approach should be taken. Another consideration when planning tests is that they should be easy to run and should complete in a reasonably short time. Such tests offer no excuses to delay running them or to not run them frequently. If those tests can be part of the software build process, and hence automated, that is an additional benefit. From the output of a test failure, it should be very clear what the issue is. If we have tests that check a single concern, or a single use case, this will help to pinpoint the failure's causes much more easily. Fortunately, all these criteria are met by the unit testing paradigm, popularized by the extreme programming developed by Kent Beck and Ron Jeffries around 1998. Quoting from \"The art of unit testing\", Kent Beck defines a unit test as A unit test is an automated piece of code that invokes a unit of work in the system and then checks a single assumption about the behavior of that unit of work. Many implementations of unit testing frameworks exist, often multiple for the same programming language. They provide a framework that takes care of the scaffolding, i.e., the main function, to run the test, the formatting of the tests output and messages, and summary reporting. As a programmer, you simply implement functions that test for specific features of your software, usually at the level of individual functions. The framework provides functions to compare expected to computed values, the result of Boolean expressions, whether exceptions have been thrown (if expected), and so on. Once the test code is written, building and running it is very easy, and can be added as a target to a make file. What to test? You will typically write unit tests that compare the return value of a function to an expected value. Good tests not only check for normal cases, but also for edge or corner cases. By way of example, consider the factorial function. One would of course write tests to verify that its return value for 3 is 6, or for 5 is 120, but the cases where it is easy to make mistakes should also be tested, in this example the input values 0 and 1 (edge cases). If the code under tests has branches, i.e., conditional statements, we should make sure that there is at least one test for each branch. The same applies to iteration statements when the number of iterations is computed, there should be tests in case no iterations have to be executed, a single iteration or multiple iterations. Another important aspect to test for is whether a function throws an exception or generates an error when it is supposed to, since this is the function's declared behavior. For the factorial example, we should verify that an error occurs when you pass to it a strictly negative argument value. Apart from providing some confidence that changes don't break our software, writing tests alongside code will actually prevent bugs, since you should really think about the behavior of your code when writing tests. It is quite probable that while doing so, you will catch bugs without even executing the unit tests. This idea is taken to the extreme in Test Driven Development (TDD). A requirement is translated into a number of tests, and only when these are implemented, the actual functions are developed. This approach is kind of satisfying: at first all tests fail, since there is a trivial implementation only. As the development progresses, and the implementation gradually nears completion, more and more tests succeed. How to test? Note that unit tests should be simple, i.e., a unit test check one particular aspect, so that failure is easy to map to its cause. This means that functions implementing unit tests are typically very short, but that we may have quite many of them. Another very important issue to keep in mind is that tests should be independent. You should be able to run them in any order without altering the results. This implies that unit tests have to be free of side effects. To summarize the characteristics of a good unit test: it is specific: failure of a test localizes a defect in a small fragment of code; it is orthogonal to other unit tests: unit tests should be independent of one another. The collection of all tests for your software project should be complete. There should be tests for all functions, but also for all code paths through your code. An aspect that is often forgotten is that you should also test for expected failure. Is an exception actually thrown, or does the exit status of a function reflect a problem? When implementing a new feature or making a change, you should of course develop tests specific to that addition or modification. However, it is not enough to simply run those new tests. Even if they fail, your code might still be broken, since an addition or a modification might introduce a bug for existing code. This situation is called a regression, since the code was correct, but due to changes, it no longer is. Hence it is important to run all tests, to ensure that there are no regressions. This practice is referred to as regression testing. If unit testing is done well, regression testing is always performed. Of course, many tests require a context to run in. Variables and data structures must have been initialized, a file should be present, and so on. Unit testing frameworks provide setup and teardown functions that can be run before and after each individual test respectively to set, or clean up the stage. These temporary artifacts are often referred to as fixtures. Unit testing frameworks typically also provide the means to group tests into so called suites, and run setup and teardown functions before running the first and after running the last test of that suite. Units tests for large projects are typically organized into multiple suites that deal with specific subsets of the code or functionality. Below you can see when various setup and teardown functions are called when test suites are executed. Examples Examples of unit testing can be found in the following repositories. Cunit for C Catch2 examples for C++ pFunit examples for Fortran pytest examples for Python","title":"Unit testing"},{"location":"testing/unit_testing/#unit-testing","text":"","title":"Unit testing"},{"location":"testing/unit_testing/#introduction","text":"Testing is obviously a way to ensure that at least part of the functionality behaves as expected. However, good tests can provide more than that, they can help ensure that changes to the code base don't introduce defects. A code base evolves dynamically, potentially over a long period of time. Adding new features to software is typically quite error prone, and might inadvertently break some use cases. In order to minimize this risk should have a sizable collection of tests available that check whether results are as expected, and be able to run those easily and frequently as part of your development cycle. Unit tests are an excellent approach. They consist of many small fragments of code that each test very specific aspects of the functionality of a library. Using frameworks such as pFUnit for Fortran, CUnit for C, Catch2 for C++, pytest for Python, or testthat for R will take care of the \"bookkeeping\" and ensure that running tests is effortless.","title":"Introduction"},{"location":"testing/unit_testing/#best-practices","text":"Very often code defects are introduced while the code evolves to implement new features or even to fix emerging problems. Without proper testing these new bugs may go unnoticed for a while, even until after the release of a new version of a software project. Hence it is good practice to have a set of tests at hand for regression testing, i.e., testing whether what used to work correctly, still does. It is a very important aspect of a software project with a non-trivial life-cycle. Testing code by running an application and visually inspecting the results is very error-prone. It is all too easy to miss a problem when the output is fairly long, as would be the case for most software projects. Hence a different approach should be taken. Another consideration when planning tests is that they should be easy to run and should complete in a reasonably short time. Such tests offer no excuses to delay running them or to not run them frequently. If those tests can be part of the software build process, and hence automated, that is an additional benefit. From the output of a test failure, it should be very clear what the issue is. If we have tests that check a single concern, or a single use case, this will help to pinpoint the failure's causes much more easily. Fortunately, all these criteria are met by the unit testing paradigm, popularized by the extreme programming developed by Kent Beck and Ron Jeffries around 1998. Quoting from \"The art of unit testing\", Kent Beck defines a unit test as A unit test is an automated piece of code that invokes a unit of work in the system and then checks a single assumption about the behavior of that unit of work. Many implementations of unit testing frameworks exist, often multiple for the same programming language. They provide a framework that takes care of the scaffolding, i.e., the main function, to run the test, the formatting of the tests output and messages, and summary reporting. As a programmer, you simply implement functions that test for specific features of your software, usually at the level of individual functions. The framework provides functions to compare expected to computed values, the result of Boolean expressions, whether exceptions have been thrown (if expected), and so on. Once the test code is written, building and running it is very easy, and can be added as a target to a make file.","title":"Best practices"},{"location":"testing/unit_testing/#what-to-test","text":"You will typically write unit tests that compare the return value of a function to an expected value. Good tests not only check for normal cases, but also for edge or corner cases. By way of example, consider the factorial function. One would of course write tests to verify that its return value for 3 is 6, or for 5 is 120, but the cases where it is easy to make mistakes should also be tested, in this example the input values 0 and 1 (edge cases). If the code under tests has branches, i.e., conditional statements, we should make sure that there is at least one test for each branch. The same applies to iteration statements when the number of iterations is computed, there should be tests in case no iterations have to be executed, a single iteration or multiple iterations. Another important aspect to test for is whether a function throws an exception or generates an error when it is supposed to, since this is the function's declared behavior. For the factorial example, we should verify that an error occurs when you pass to it a strictly negative argument value. Apart from providing some confidence that changes don't break our software, writing tests alongside code will actually prevent bugs, since you should really think about the behavior of your code when writing tests. It is quite probable that while doing so, you will catch bugs without even executing the unit tests. This idea is taken to the extreme in Test Driven Development (TDD). A requirement is translated into a number of tests, and only when these are implemented, the actual functions are developed. This approach is kind of satisfying: at first all tests fail, since there is a trivial implementation only. As the development progresses, and the implementation gradually nears completion, more and more tests succeed.","title":"What to test?"},{"location":"testing/unit_testing/#how-to-test","text":"Note that unit tests should be simple, i.e., a unit test check one particular aspect, so that failure is easy to map to its cause. This means that functions implementing unit tests are typically very short, but that we may have quite many of them. Another very important issue to keep in mind is that tests should be independent. You should be able to run them in any order without altering the results. This implies that unit tests have to be free of side effects. To summarize the characteristics of a good unit test: it is specific: failure of a test localizes a defect in a small fragment of code; it is orthogonal to other unit tests: unit tests should be independent of one another. The collection of all tests for your software project should be complete. There should be tests for all functions, but also for all code paths through your code. An aspect that is often forgotten is that you should also test for expected failure. Is an exception actually thrown, or does the exit status of a function reflect a problem? When implementing a new feature or making a change, you should of course develop tests specific to that addition or modification. However, it is not enough to simply run those new tests. Even if they fail, your code might still be broken, since an addition or a modification might introduce a bug for existing code. This situation is called a regression, since the code was correct, but due to changes, it no longer is. Hence it is important to run all tests, to ensure that there are no regressions. This practice is referred to as regression testing. If unit testing is done well, regression testing is always performed. Of course, many tests require a context to run in. Variables and data structures must have been initialized, a file should be present, and so on. Unit testing frameworks provide setup and teardown functions that can be run before and after each individual test respectively to set, or clean up the stage. These temporary artifacts are often referred to as fixtures. Unit testing frameworks typically also provide the means to group tests into so called suites, and run setup and teardown functions before running the first and after running the last test of that suite. Units tests for large projects are typically organized into multiple suites that deal with specific subsets of the code or functionality. Below you can see when various setup and teardown functions are called when test suites are executed.","title":"How to test?"},{"location":"testing/unit_testing/#examples","text":"Examples of unit testing can be found in the following repositories. Cunit for C Catch2 examples for C++ pFunit examples for Fortran pytest examples for Python","title":"Examples"},{"location":"tools/","text":"Tools This section contains a list of tools that you may find useful when doing software development in a scientific context. Some tools are general, i.e., they are useful for any programming langage, while others are specific to a particular language. General Tools C C++ Fortran Python R","title":"Overview"},{"location":"tools/#tools","text":"This section contains a list of tools that you may find useful when doing software development in a scientific context. Some tools are general, i.e., they are useful for any programming langage, while others are specific to a particular language. General Tools C C++ Fortran Python R","title":"Tools"},{"location":"tools/C-plus-plus/","text":"Tools for C++ programming Code formatting clang-format : code formatter is part of the Clang Tools. Linting and static analysis clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++. Configuration files libconfig : configuration file management library for C++. Package managers Conan : package manager for C and C++. vspkg : package manager for C and C++. Testing Catch2 : C++ unit testing framework that also supports Behavior-Driven Development (BDD). Profiling gprof is a free profiler. Intel VTune is a commercial profiler that is part of Intel OneAPI. Arm Forge is a commercial profiler and debugger that is specifically designed for HPC. It can profile OpenMP, MPI, and CUDA code. Scalasca is an open-source profiler that can profile MPI and OpenMP code.","title":"C++"},{"location":"tools/C-plus-plus/#tools-for-c-programming","text":"","title":"Tools for C++ programming"},{"location":"tools/C-plus-plus/#code-formatting","text":"clang-format : code formatter is part of the Clang Tools.","title":"Code formatting"},{"location":"tools/C-plus-plus/#linting-and-static-analysis","text":"clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++.","title":"Linting and static analysis"},{"location":"tools/C-plus-plus/#configuration-files","text":"libconfig : configuration file management library for C++.","title":"Configuration files"},{"location":"tools/C-plus-plus/#package-managers","text":"Conan : package manager for C and C++. vspkg : package manager for C and C++.","title":"Package managers"},{"location":"tools/C-plus-plus/#testing","text":"Catch2 : C++ unit testing framework that also supports Behavior-Driven Development (BDD).","title":"Testing"},{"location":"tools/C-plus-plus/#profiling","text":"gprof is a free profiler. Intel VTune is a commercial profiler that is part of Intel OneAPI. Arm Forge is a commercial profiler and debugger that is specifically designed for HPC. It can profile OpenMP, MPI, and CUDA code. Scalasca is an open-source profiler that can profile MPI and OpenMP code.","title":"Profiling"},{"location":"tools/C/","text":"Tools for C programming Code formatting clang-format : code formatter is part of the Clang Tools. Linting and static analysis clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++. Configuration files libconfig : configuration file management library for C. Package managers Conan : package manager for C and C++. vspkg : package manager for C and C++. Testing CUnit : unit testing framework for C. Profiling gprof is a free profiler. Intel VTune is a commercial profiler that is part of Intel OneAPI. Arm Forge is a commercial profiler and debugger that is specifically designed for HPC. It can profile OpenMP, MPI, and CUDA code. Scalasca is an open-source profiler that can profile MPI and OpenMP code.","title":"C"},{"location":"tools/C/#tools-for-c-programming","text":"","title":"Tools for C programming"},{"location":"tools/C/#code-formatting","text":"clang-format : code formatter is part of the Clang Tools.","title":"Code formatting"},{"location":"tools/C/#linting-and-static-analysis","text":"clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++.","title":"Linting and static analysis"},{"location":"tools/C/#configuration-files","text":"libconfig : configuration file management library for C.","title":"Configuration files"},{"location":"tools/C/#package-managers","text":"Conan : package manager for C and C++. vspkg : package manager for C and C++.","title":"Package managers"},{"location":"tools/C/#testing","text":"CUnit : unit testing framework for C.","title":"Testing"},{"location":"tools/C/#profiling","text":"gprof is a free profiler. Intel VTune is a commercial profiler that is part of Intel OneAPI. Arm Forge is a commercial profiler and debugger that is specifically designed for HPC. It can profile OpenMP, MPI, and CUDA code. Scalasca is an open-source profiler that can profile MPI and OpenMP code.","title":"Profiling"},{"location":"tools/Fortran/","text":"Tools for Fortran programming Testing pFUnit : is a very rich framework for developing unit tests. Profiling gprof is a free profiler. Intel VTune is a commercial profiler that is part of Intel OneAPI. Arm Forge is a commercial profiler and debugger that is specifically designed for HPC. It can profile OpenMP, MPI, and CUDA code. Scalasca is an open-source profiler that can profile MPI and OpenMP code. Build tools FPM : Fortran Package Manager (FPM) helps you create Fortran software project, initializing the project directory with the appropriate directories, configuration files and such. It also supports managing dependencies, running your code and tests.","title":"Fortran"},{"location":"tools/Fortran/#tools-for-fortran-programming","text":"","title":"Tools for Fortran programming"},{"location":"tools/Fortran/#testing","text":"pFUnit : is a very rich framework for developing unit tests.","title":"Testing"},{"location":"tools/Fortran/#profiling","text":"gprof is a free profiler. Intel VTune is a commercial profiler that is part of Intel OneAPI. Arm Forge is a commercial profiler and debugger that is specifically designed for HPC. It can profile OpenMP, MPI, and CUDA code. Scalasca is an open-source profiler that can profile MPI and OpenMP code.","title":"Profiling"},{"location":"tools/Fortran/#build-tools","text":"FPM : Fortran Package Manager (FPM) helps you create Fortran software project, initializing the project directory with the appropriate directories, configuration files and such. It also supports managing dependencies, running your code and tests.","title":"Build tools"},{"location":"tools/Python/","text":"Tools for Python programming Code formatting Black : code formatter for Python (very opinionated). Linting and static analysis flake8 : tool to enforce adherence to the Python style guide. mypy : if your code has type annotations, mypy will check for correctness. pylint : static code analyzer that also checks for documentation and style. Package managers miniconda : Python package manager. mamba : Python package manager. Poetry : environment management, build and deployment of Python projects. Configuration files Hydra : configuration management tool that allows you to compose complex configurations from simple pieces. Testing pytest : unit testing framework. hypothesis : property-based testing for Python, it also allows for fuzz testing. Profiling CProfile is a profiler that is part of Python's standard library. snakeviz is a viewer for cProfile output. line_profiler is a line-by-line profiler. memory_profiler is a memory profiler, it allows how much memory is used line-by-line in your code. Language interoperability Cython : a superset of Python that allows you to write C extensions for Python. Pybind11 : a header-only library that exposes C++ types in Python. SWIG : a tool that generates C++ wrappers for Python. Scikit-build : a build system generator for CMake that allows you to build C++ extensions for Python. F2PY3 : a tool that allows you to wrap Fortran code for Python. Build tools Poetry : environment management, build and deployment of Python projects.","title":"Python"},{"location":"tools/Python/#tools-for-python-programming","text":"","title":"Tools for Python programming"},{"location":"tools/Python/#code-formatting","text":"Black : code formatter for Python (very opinionated).","title":"Code formatting"},{"location":"tools/Python/#linting-and-static-analysis","text":"flake8 : tool to enforce adherence to the Python style guide. mypy : if your code has type annotations, mypy will check for correctness. pylint : static code analyzer that also checks for documentation and style.","title":"Linting and static analysis"},{"location":"tools/Python/#package-managers","text":"miniconda : Python package manager. mamba : Python package manager. Poetry : environment management, build and deployment of Python projects.","title":"Package managers"},{"location":"tools/Python/#configuration-files","text":"Hydra : configuration management tool that allows you to compose complex configurations from simple pieces.","title":"Configuration files"},{"location":"tools/Python/#testing","text":"pytest : unit testing framework. hypothesis : property-based testing for Python, it also allows for fuzz testing.","title":"Testing"},{"location":"tools/Python/#profiling","text":"CProfile is a profiler that is part of Python's standard library. snakeviz is a viewer for cProfile output. line_profiler is a line-by-line profiler. memory_profiler is a memory profiler, it allows how much memory is used line-by-line in your code.","title":"Profiling"},{"location":"tools/Python/#language-interoperability","text":"Cython : a superset of Python that allows you to write C extensions for Python. Pybind11 : a header-only library that exposes C++ types in Python. SWIG : a tool that generates C++ wrappers for Python. Scikit-build : a build system generator for CMake that allows you to build C++ extensions for Python. F2PY3 : a tool that allows you to wrap Fortran code for Python.","title":"Language interoperability"},{"location":"tools/Python/#build-tools","text":"Poetry : environment management, build and deployment of Python projects.","title":"Build tools"},{"location":"tools/R/","text":"Tools for R programmers Testing testthat : testing framework for R. Software deployment usethis : takes care of repetitive task associated with package setup and R development more generally. Language interoperability Rcpp : allows you to write C++ code that can be called from R.","title":"R"},{"location":"tools/R/#tools-for-r-programmers","text":"","title":"Tools for R programmers"},{"location":"tools/R/#testing","text":"testthat : testing framework for R.","title":"Testing"},{"location":"tools/R/#software-deployment","text":"usethis : takes care of repetitive task associated with package setup and R development more generally.","title":"Software deployment"},{"location":"tools/R/#language-interoperability","text":"Rcpp : allows you to write C++ code that can be called from R.","title":"Language interoperability"},{"location":"tools/general/","text":"Programming language-agnostic tools This is a list of tools that are not specific to a specific programming language. Version control git : version control software. SmartGit : nice git GUI application that is cross-platform (Windows/MacOS/Linux) and can be used with multiple hosting services. DVC : Data Version Control, tool to manage versions of your data alongside your code. Using this tool avoid having to commit large files into a git repository for which it is not intended. GitHub hosting platform. GitLab hosting platform. Build tools Some build tools can be used for many programming languages. Build tools let you specify how to build your applications and libraries, but often also how to test and package them. CMake : cross-platform build tool that can be used to build and install C/C++/Fortran libraries and applications. A number of examples of using CMake in various scenarios can be found in the repository on CMake use cases . Package managers Some package managers are fairly specific to a particular programming language, but others can be used for multiple languages, or even workflows. Spack : package manager that can be used to install software in a controlled environment, ensuring that the software is built with the correct dependencies and compiler options. Libraries Libraries are collections of code that can be used to perform specific tasks. The libraries listed here are useful in the context of scientific computing. They are available in multiple programming languages via bindings, or used under the hood by language-specific libraries. BLAS : Basic Linear Algebra Subprograms, a library that provides standard building blocks for performing linear algebra operations. LAPACK : Linear Algebra PACKage, a library that provides routines for solving systems of linear equations, linear least squares problems, eigenvalue problems, and singular value decomposition. FFTW : Fastest Fourier Transform in the West, a library that provides routines for computing the Discrete Fourier Transform (DFT) and its inverse. Containers Containers are a way to package your code and its dependencies in a way that makes it easy to share with others. They can be used to ensure that your code runs in the same environment on different machines. In addition, they can be used to ensure that your code runs in the same environment at different times, contributing to the reproducibility of your results. Docker : containerization tool that can be used to specify the exact environment in which your code is run, including the operating system and the versions of all software used. Podman : a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Apptainer : containerization tool that is compatible with Singularity and Docker. It is designed to be used in an HPC environment and doesn't require root privileges to build images and run containers. Workflows Snakemake : a workflow management system that aims to reduce the complexity of creating workflows by providing a readable and expressive syntax in Python style. Nextflow : a workflow manager that enables the development of portable and reproducible workflows. It comes with a domain-specific language that simplifies the writing of complex computational workflows. Documentation In this session, we will discuss two tools for creating attractive documentation, Doxygen and MkDocs. The former is best suited for reference guides, while the latter is excellent for tutorial-style material. Doxygen : some programming languages such as Java and Python provide support for documentation as part of their specification. The languages we use most frequently in an HPC context, C, C++, and Fortran, have no such provisions. However, Doxygen generates reference documentation out of comment blocks for a wide variety of programming languages, including those of interest to us. This documentation is fully hyperlinked. For instance, clicking the type of a function's argument will bring you to the type's documentation. MkDocs : this is a very convenient tool for generating nice looking documentation that can be viewed standalone as HTML pages, or that can be served from the ReadTheDocs service. It automatically generates a navigation panel and adds search functionality. You can also define a GitHub trigger that will automatically push your project's documentation to Read the Docs each time you do a release. Documentation of previous software versions remain available. In that scenario, MkDocs will provide useful previews before you make a release of your code project. Sphinx : this is another tool to generate documentation. It can generate API reference documentation for Python, and tutorial style documentation in general. The resulting documentation can be hosted on ReadTheDocs or GitHub pages. ReadTheDocs : a hosting service for documentation. It supports both MkDocs and Sphinx. Documentation can be fetched from a GitHub repository and (re)built. This can be automized and set to be triggered by, e.g., a merge into main. GitHub pages : you can activate pages for any GitHub repository. This will create a website that you can use to host the documentation for your project to make it available to your group or even to every user of your software. The documentaiton can be genreated using a GitHub Action triggered by, for instance, a merge into the main branch. The repository that hosts this information is an example of that. Testing Some testing tools are generic and can be used to do functional testing for application developed in any programming language. shunit2 framework : use Bash scripts to perform functional testing as unit tests. CTest : CTest is part of CMake and lets you do functional testing as part of the build process. Licensing Selecting an appropirate license is not trivial. Website that tries to guide you through the process. Attribution Zenodo : website to request a Digital Object Identifier (DOI) for your version control repositories.","title":"General tools"},{"location":"tools/general/#programming-language-agnostic-tools","text":"This is a list of tools that are not specific to a specific programming language.","title":"Programming language-agnostic tools"},{"location":"tools/general/#version-control","text":"git : version control software. SmartGit : nice git GUI application that is cross-platform (Windows/MacOS/Linux) and can be used with multiple hosting services. DVC : Data Version Control, tool to manage versions of your data alongside your code. Using this tool avoid having to commit large files into a git repository for which it is not intended. GitHub hosting platform. GitLab hosting platform.","title":"Version control"},{"location":"tools/general/#build-tools","text":"Some build tools can be used for many programming languages. Build tools let you specify how to build your applications and libraries, but often also how to test and package them. CMake : cross-platform build tool that can be used to build and install C/C++/Fortran libraries and applications. A number of examples of using CMake in various scenarios can be found in the repository on CMake use cases .","title":"Build tools"},{"location":"tools/general/#package-managers","text":"Some package managers are fairly specific to a particular programming language, but others can be used for multiple languages, or even workflows. Spack : package manager that can be used to install software in a controlled environment, ensuring that the software is built with the correct dependencies and compiler options.","title":"Package managers"},{"location":"tools/general/#libraries","text":"Libraries are collections of code that can be used to perform specific tasks. The libraries listed here are useful in the context of scientific computing. They are available in multiple programming languages via bindings, or used under the hood by language-specific libraries. BLAS : Basic Linear Algebra Subprograms, a library that provides standard building blocks for performing linear algebra operations. LAPACK : Linear Algebra PACKage, a library that provides routines for solving systems of linear equations, linear least squares problems, eigenvalue problems, and singular value decomposition. FFTW : Fastest Fourier Transform in the West, a library that provides routines for computing the Discrete Fourier Transform (DFT) and its inverse.","title":"Libraries"},{"location":"tools/general/#containers","text":"Containers are a way to package your code and its dependencies in a way that makes it easy to share with others. They can be used to ensure that your code runs in the same environment on different machines. In addition, they can be used to ensure that your code runs in the same environment at different times, contributing to the reproducibility of your results. Docker : containerization tool that can be used to specify the exact environment in which your code is run, including the operating system and the versions of all software used. Podman : a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Apptainer : containerization tool that is compatible with Singularity and Docker. It is designed to be used in an HPC environment and doesn't require root privileges to build images and run containers.","title":"Containers"},{"location":"tools/general/#workflows","text":"Snakemake : a workflow management system that aims to reduce the complexity of creating workflows by providing a readable and expressive syntax in Python style. Nextflow : a workflow manager that enables the development of portable and reproducible workflows. It comes with a domain-specific language that simplifies the writing of complex computational workflows.","title":"Workflows"},{"location":"tools/general/#documentation","text":"In this session, we will discuss two tools for creating attractive documentation, Doxygen and MkDocs. The former is best suited for reference guides, while the latter is excellent for tutorial-style material. Doxygen : some programming languages such as Java and Python provide support for documentation as part of their specification. The languages we use most frequently in an HPC context, C, C++, and Fortran, have no such provisions. However, Doxygen generates reference documentation out of comment blocks for a wide variety of programming languages, including those of interest to us. This documentation is fully hyperlinked. For instance, clicking the type of a function's argument will bring you to the type's documentation. MkDocs : this is a very convenient tool for generating nice looking documentation that can be viewed standalone as HTML pages, or that can be served from the ReadTheDocs service. It automatically generates a navigation panel and adds search functionality. You can also define a GitHub trigger that will automatically push your project's documentation to Read the Docs each time you do a release. Documentation of previous software versions remain available. In that scenario, MkDocs will provide useful previews before you make a release of your code project. Sphinx : this is another tool to generate documentation. It can generate API reference documentation for Python, and tutorial style documentation in general. The resulting documentation can be hosted on ReadTheDocs or GitHub pages. ReadTheDocs : a hosting service for documentation. It supports both MkDocs and Sphinx. Documentation can be fetched from a GitHub repository and (re)built. This can be automized and set to be triggered by, e.g., a merge into main. GitHub pages : you can activate pages for any GitHub repository. This will create a website that you can use to host the documentation for your project to make it available to your group or even to every user of your software. The documentaiton can be genreated using a GitHub Action triggered by, for instance, a merge into the main branch. The repository that hosts this information is an example of that.","title":"Documentation"},{"location":"tools/general/#testing","text":"Some testing tools are generic and can be used to do functional testing for application developed in any programming language. shunit2 framework : use Bash scripts to perform functional testing as unit tests. CTest : CTest is part of CMake and lets you do functional testing as part of the build process.","title":"Testing"},{"location":"tools/general/#licensing","text":"Selecting an appropirate license is not trivial. Website that tries to guide you through the process.","title":"Licensing"},{"location":"tools/general/#attribution","text":"Zenodo : website to request a Digital Object Identifier (DOI) for your version control repositories.","title":"Attribution"}]}